{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "base_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1949_33905_id=cp131507_linz.jpg',\n",
       " '0711_4263_id=cp131504_linz.jpg',\n",
       " '0271_1458_id=cp131506_linz.jpg',\n",
       " '0606_3010_id=cp131502_linz.jpg',\n",
       " '0791_4970_id=cp131503_linz.jpg',\n",
       " '1950_33912_id=cp131509_linz.jpg',\n",
       " '1141_8563_id=cp131500_linz.jpg',\n",
       " '0270_1455_id=cp131505_linz.jpg',\n",
       " '1153_8691_id=cp131501_linz.jpg',\n",
       " '1950_33908_id=cp131508_linz.jpg']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "munich_imgs = os.listdir(\"scraped_images_grayscaled_big\")\n",
    "furnitures = [r for r in munich_imgs if \"linz\" and \"cp13150\" in r ]\n",
    "furnitures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#vgg models (16 and 19 similar, but 16 is better)\n",
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "# model = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "model.features[0] = nn.Conv2d(1,64,kernel_size=(3,3), stride=(1,1),padding=(1,1))\n",
    "model = nn.Sequential(*[*list(model.children())[:-1][0][:-10]])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    This function takes a path to a single image, it then resizes it to size 50x50 \\\n",
    "    and normalizes it to the range [0,1]. Lastly, it adds an extra dimension to the image \\\n",
    "    which represents the batch size. These steps are needed, because we want to pass the image \\\n",
    "    to a CNN. \n",
    "    \"\"\"\n",
    "    \n",
    "    img = cv2.imread(image_path, -1)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8,8))\n",
    "    img = clahe.apply(img)\n",
    "    _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    mask = np.ones(img.shape, np.uint8)\n",
    "    mask.fill(255)\n",
    "    cv2.drawContours(mask, contours, 0, 0, -1)\n",
    "    img = cv2.add(thresh, mask)\n",
    "    kernel = np.ones((5,5), dtype=np.uint8)\n",
    "    img = cv2.erode(img, kernel, 10)\n",
    "    img = np.abs(np.max(img) - img)\n",
    "    \n",
    "    img = cv2.resize(img, (50, 50), interpolation=cv2.INTER_AREA)\n",
    "    preprocess = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    img = preprocess(img).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "def extract_features(image_path):\n",
    "    \"\"\"\n",
    "    This function takes a path to a single image, it then preprocesses the image with the \\\n",
    "    function preprocess_image. Afterwards it passes the image to the pretrained CNN to extract \\\n",
    "    a feature descriptor. \n",
    "    \"\"\"\n",
    "    \n",
    "    img = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "    return features.squeeze(0).numpy()\n",
    "\n",
    "def normalize_features(features):\n",
    "    \"\"\"\n",
    "    This function takes the feature descriptor and normalizes it. This is needed as we want \\\n",
    "    to compute the dot-product similarity between feature descriptors of different images. \\\n",
    "    And for similarity it is convenient to have all pixels on the same scale without too \\\n",
    "    much magnitude differences and this also ensures stability. \n",
    "    \"\"\"\n",
    "    \n",
    "    return features / np.linalg.norm(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resnet models (gives high scores regardless)\n",
    "# # model = torchvision.models.resnet50(pretrained=True)\n",
    "# # model = torchvision.models.resnet101(pretrained=True) \n",
    "# # model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "# # model = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     This function takes a path to a single image, it then resizes it to size 50x50 \\\n",
    "#     and normalizes it to the range [0,1]. Lastly, it adds an extra dimension to the image \\\n",
    "#     which represents the batch size. These steps are needed, because we want to pass the image \\\n",
    "#     to a CNN. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     img = cv2.imread(image_path, -1)\n",
    "#     clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8,8))\n",
    "#     img = clahe.apply(img)\n",
    "#     _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "#     contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "#     mask = np.ones(img.shape, np.uint8)\n",
    "#     mask.fill(255)\n",
    "#     cv2.drawContours(mask, contours, 0, 0, -1)\n",
    "#     img = cv2.add(thresh, mask)\n",
    "#     kernel = np.ones((5,5), dtype=np.uint8)\n",
    "#     img = cv2.erode(img, kernel, 10)\n",
    "#     img = np.abs(np.max(img) - img)\n",
    "    \n",
    "#     img = cv2.resize(img, (50, 50), interpolation=cv2.INTER_AREA)\n",
    "#     preprocess = torchvision.transforms.Compose([\n",
    "#         torchvision.transforms.ToTensor()\n",
    "#     ])\n",
    "#     img = preprocess(img).unsqueeze(0)\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     This function takes a path to a single image, it then preprocesses the image with the \\\n",
    "#     function preprocess_image. Afterwards it passes the image to the pretrained CNN to extract \\\n",
    "#     a feature descriptor. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     img = preprocess_image(image_path)\n",
    "#     with torch.no_grad():\n",
    "#         features = model(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     This function takes the feature descriptor and normalizes it. This is needed as we want \\\n",
    "#     to compute the dot-product similarity between feature descriptors of different images. \\\n",
    "#     And for similarity it is convenient to have all pixels on the same scale without too \\\n",
    "#     much magnitude differences and this also ensures stability. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     return features / np.linalg.norm(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # InceptionV3 model (potential)\n",
    "# model = torchvision.models.inception_v3(pretrained=True, aux_logits=True)\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     Preprocesses a single image for InceptionV3 input.\n",
    "#     \"\"\"\n",
    "#     # img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image as grayscale\n",
    "#     # img = cv2.resize(img, (299, 299))  # Resize to 299x299 for InceptionV3\n",
    "#     # img = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "#     # # Since the model expects 3 channels, we stack the grayscale image to create 3 channels\n",
    "#     # img = np.stack([img, img, img], axis=-1)\n",
    "    \n",
    "#     # img = (img - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
    "#     # img = np.transpose(img, (2, 0, 1))  # Transpose to (channels, height, width)\n",
    "#     # img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "#     # img = torch.tensor(img, dtype=torch.float32)  # Convert to PyTorch tensor with dtype float32\n",
    "#         # Read the image as grayscale\n",
    "#     img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "#     # Resize to 299x299 for InceptionV3\n",
    "#     img = cv2.resize(img, (299, 299))\n",
    "    \n",
    "#     # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "#     clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "#     img = clahe.apply(img)\n",
    "    \n",
    "#     # Normalize to [0, 1]\n",
    "#     img = np.array(img, dtype=np.float32) / 255.0\n",
    "    \n",
    "#     # Since the model expects 3 channels, we stack the grayscale image to create 3 channels\n",
    "#     img = np.stack([img, img, img], axis=-1)\n",
    "    \n",
    "#     # Apply Gamma correction\n",
    "#     gamma = 1.2  # Example gamma value, adjust as necessary\n",
    "#     img = np.power(img, gamma)\n",
    "    \n",
    "#     # Normalize with ImageNet mean and std\n",
    "#     img = (img - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "#     # Transpose to (channels, height, width)\n",
    "#     img = np.transpose(img, (2, 0, 1))\n",
    "    \n",
    "#     # Add batch dimension\n",
    "#     img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "#     # Convert to PyTorch tensor with dtype float32\n",
    "#     img = torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     Extract features from a single image using InceptionV3.\n",
    "#     \"\"\"\n",
    "#     img = preprocess_image(image_path)\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():\n",
    "#         features = model(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     Normalize the extracted features.\n",
    "#     \"\"\"\n",
    "#     return features / np.linalg.norm(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mobile net v2 (gives to high of scores to everything)\n",
    "# model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# # Modify the first convolutional layer to accept grayscale images\n",
    "# model.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     Preprocesses a single grayscale image for MobileNetV2 input.\n",
    "#     \"\"\"\n",
    "#     img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image as grayscale\n",
    "#     img = cv2.resize(img, (224, 224))  # Resize to 224x224 for MobileNetV2\n",
    "#     img = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "#     img = np.expand_dims(img, axis=0)  # Add channel dimension\n",
    "#     img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "#     img = torch.tensor(img, dtype=torch.float32)  # Convert to PyTorch tensor with dtype float32\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     Extract features from a single image using MobileNetV2.\n",
    "#     \"\"\"\n",
    "#     img = preprocess_image(image_path)\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():\n",
    "#         features = model.features(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     Normalize the extracted features.\n",
    "#     \"\"\"\n",
    "#     return features / np.linalg.norm(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DenseNet (gives similar scores to everything)\n",
    "# model = torchvision.models.densenet121(pretrained=True)\n",
    "\n",
    "# # Modify the first convolutional layer to accept grayscale images\n",
    "# model.features.conv0 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     Preprocesses a single grayscale image for DenseNet input.\n",
    "#     \"\"\"\n",
    "#     img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image as grayscale\n",
    "#     img = cv2.resize(img, (224, 224))  # Resize to 224x224 for DenseNet\n",
    "#     img = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "#     img = np.expand_dims(img, axis=0)  # Add channel dimension\n",
    "#     img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "#     img = torch.tensor(img, dtype=torch.float32)  # Convert to PyTorch tensor with dtype float32\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     Extract features from a single image using DenseNet.\n",
    "#     \"\"\"\n",
    "#     img = preprocess_image(image_path)\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():\n",
    "#         features = model.features(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     Normalize the extracted features.\n",
    "#     \"\"\"\n",
    "#     return features / np.linalg.norm(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alexnet (gives good scores unconditionally)\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "# # Modify the first convolutional layer to accept grayscale images\n",
    "# model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     Preprocesses a single grayscale image for AlexNet input.\n",
    "#     \"\"\"\n",
    "#     img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image as grayscale\n",
    "#     img = cv2.resize(img, (224, 224))  # Resize to 224x224 for AlexNet\n",
    "#     img = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "#     img = np.expand_dims(img, axis=0)  # Add channel dimension\n",
    "#     img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "#     img = torch.tensor(img, dtype=torch.float32)  # Convert to PyTorch tensor with dtype float32\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     Extract features from a single image using AlexNet.\n",
    "#     \"\"\"\n",
    "#     img = preprocess_image(image_path)\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():\n",
    "#         features = model.features(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     Normalize the extracted features.\n",
    "#     \"\"\"\n",
    "#     return features / np.linalg.norm(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities_testsets(munich_testset, nk_testset, \n",
    "                                  munich_path=\"munich_test_no_back\", \n",
    "                                  nk_path=\"nk_test_no_back\"):\n",
    "    \"\"\"\n",
    "    This function takes four arguments: \n",
    "    - munich_testset, which contains 5 grayscaled images from the munich database.\n",
    "    - nk_testset, which contains 5 grayscaled images from the nk collection API.\n",
    "    - munich path, the path to the directory of the munich images. \n",
    "    - nk_path, the path to the directory of the nk images. \n",
    "    \n",
    "    It then computes the feature descriptors for the munich images and all the \\\n",
    "    nk collection images. Afterwards takes the dot-product to get the dot-product similiarity. \n",
    "    It then saves the similarity and the two images as key-value pairs in a dictionary. \n",
    "    \"\"\"\n",
    "    \n",
    "    similarities = {}\n",
    "    for nk_img in nk_testset:\n",
    "        nk_img_path = os.path.join(nk_path, nk_img)\n",
    "        for munich_img in munich_testset:\n",
    "            munich_img_path = os.path.join(munich_path, munich_img)\n",
    "            nk_img_feature_descriptor = normalize_features(extract_features(nk_img_path).flatten())\n",
    "            munich_img_feature_descriptor = normalize_features(extract_features(munich_img_path).flatten())\n",
    "            similarity = np.dot(\n",
    "                nk_img_feature_descriptor,\n",
    "                munich_img_feature_descriptor\n",
    "            )\n",
    "            similarities[(nk_img, munich_img)] = similarity.item()\n",
    "        \n",
    "    return similarities\n",
    "\n",
    "def compute_similarities_img_to_set(nk_img, munich_testset, \n",
    "                                    munich_path=\"mc_no_back\", \n",
    "                                    nk_path=\"nk_no_back\"):\n",
    "    similarities = {}\n",
    "    nk_img_path = os.path.join(nk_path, nk_img)\n",
    "    \n",
    "    # Ensure the NK image file exists\n",
    "    if not os.path.exists(nk_img_path):\n",
    "        raise FileNotFoundError(f\"NK image file {nk_img_path} not found.\")\n",
    "    \n",
    "    nk_img_feature_descriptor = normalize_features(extract_features(nk_img_path).flatten())\n",
    "    \n",
    "    for munich_img in munich_testset:\n",
    "        munich_img_path = os.path.join(munich_path, munich_img)\n",
    "        \n",
    "        # Ensure the Munich image file exists\n",
    "        if not os.path.exists(munich_img_path):\n",
    "            raise FileNotFoundError(f\"Munich image file {munich_img_path} not found.\")\n",
    "        \n",
    "        munich_img_feature_descriptor = normalize_features(extract_features(munich_img_path).flatten())\n",
    "        similarity = np.dot(\n",
    "            nk_img_feature_descriptor,\n",
    "            munich_img_feature_descriptor\n",
    "        )\n",
    "        similarities[(nk_img, munich_img)] = similarity.item()\n",
    "        \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /io/opencv/modules/imgproc/src/clahe.cpp:353: error: (-215:Assertion failed) _src.type() == CV_8UC1 || _src.type() == CV_16UC1 in function 'apply'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m nk_testset \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnk_test_no_back\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m munich_testset \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmunich_test_no_back\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m sims_test \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_similarities_testsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmunich_testset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnk_testset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m sims_test\n",
      "Cell \u001b[0;32mIn[24], line 21\u001b[0m, in \u001b[0;36mcompute_similarities_testsets\u001b[0;34m(munich_testset, nk_testset, munich_path, nk_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m munich_img \u001b[38;5;129;01min\u001b[39;00m munich_testset:\n\u001b[1;32m     20\u001b[0m     munich_img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(munich_path, munich_img)\n\u001b[0;32m---> 21\u001b[0m     nk_img_feature_descriptor \u001b[38;5;241m=\u001b[39m normalize_features(\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnk_img_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m     22\u001b[0m     munich_img_feature_descriptor \u001b[38;5;241m=\u001b[39m normalize_features(extract_features(munich_img_path)\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m     23\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\n\u001b[1;32m     24\u001b[0m         nk_img_feature_descriptor,\n\u001b[1;32m     25\u001b[0m         munich_img_feature_descriptor\n\u001b[1;32m     26\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[17], line 44\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(image_path):\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    This function takes a path to a single image, it then preprocesses the image with the \\\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    function preprocess_image. Afterwards it passes the image to the pretrained CNN to extract \\\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    a feature descriptor. \u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     46\u001b[0m         features \u001b[38;5;241m=\u001b[39m model(img)\n",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m clahe \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcreateCLAHE(clipLimit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.5\u001b[39m, tileGridSize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m---> 18\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mclahe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m _, thresh \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mthreshold(img, \u001b[38;5;241m127\u001b[39m, \u001b[38;5;241m255\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mTHRESH_BINARY)\n\u001b[1;32m     20\u001b[0m contours, hierarchy \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mfindContours(thresh, cv2\u001b[38;5;241m.\u001b[39mRETR_TREE, cv2\u001b[38;5;241m.\u001b[39mCHAIN_APPROX_SIMPLE)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /io/opencv/modules/imgproc/src/clahe.cpp:353: error: (-215:Assertion failed) _src.type() == CV_8UC1 || _src.type() == CV_16UC1 in function 'apply'\n"
     ]
    }
   ],
   "source": [
    "nk_testset = os.listdir(\"nk_test_no_back\")\n",
    "munich_testset = os.listdir(\"munich_test_no_back\")\n",
    "\n",
    "sims_test = compute_similarities_testsets(munich_testset, nk_testset)\n",
    "sims_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munich_set = os.listdir(\"scraped_images_grayscaled_big\")\n",
    "nk_img = (\"speeltafel_nk.png\")\n",
    "\n",
    "sims = compute_similarities_img_to_set(nk_img, munich_set, munich_path='scraped_images_grayscaled_big' ,nk_path='nk_test_no_back')\n",
    "# print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_table(sims):\n",
    "#     \"\"\"\n",
    "#     This function takes the output produced by either the compute_similarities \\ \n",
    "#     or compute_similarities_testsets function, and returns a pandas dataframe/table \\\n",
    "#     and also saves it in excel.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     data = {}\n",
    "#     rows = []\n",
    "\n",
    "#     for key, value in sims.items():\n",
    "#         if key[0] not in data:\n",
    "#             data[key[0]] = []\n",
    "#         if key[1] not in rows:\n",
    "#             rows.append(key[1])\n",
    "#         data[key[0]].append(value)\n",
    "        \n",
    "#     data = {key[:key.rfind(\".\")]:value for key, value in data.items()}\n",
    "#     rows = [row[:row.rfind(\".\")] for row in rows]\n",
    "        \n",
    "#     df = pd.DataFrame(data, index=rows)\n",
    "#     #df.to_excel('output.xlsx')\n",
    "#     return df.T\n",
    "    \n",
    "# get_table(sims_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speeltafel_nk.png': [('0543_2543_id=cp151474_badv.jpg', 0.5409100651741028),\n",
       "  ('0543_2543_id=cp151473_badv.jpg', 0.5401169657707214),\n",
       "  ('2075_41133_id=cp159122_badv.jpg', 0.5321546196937561),\n",
       "  ('2075_41136_id=cp159123_badv.jpg', 0.5199837684631348),\n",
       "  ('1110_8010-17_id=cp174107_badv.jpg', 0.5186924338340759),\n",
       "  ('0238_1286-1_id=cp139334_badv.jpg', 0.5139502882957458),\n",
       "  ('2152_43796_id=cp162079_badv.jpg', 0.48791271448135376),\n",
       "  ('2278_46800_id=cp164916_badv.jpg', 0.4876925051212311),\n",
       "  ('2152_43797_id=cp162080_badv.jpg', 0.48460230231285095),\n",
       "  ('1554_13932_id=cp140925_badv.jpg', 0.48370492458343506)]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_10_similarities(similarities):\n",
    "    \"\"\"\n",
    "    Get the top 10 highest similarity values for each NK image from the similarities dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - similarities: A dictionary where keys are (NK image name, Munich image name) tuples\n",
    "                    and values are similarity scores.\n",
    "                    \n",
    "    Returns:\n",
    "    - A dictionary where keys are NK image names and values are lists of tuples\n",
    "      (Munich image name, similarity) sorted by similarity in descending order.\n",
    "    \"\"\"\n",
    "    top_10_similarities = {}\n",
    "    \n",
    "    for nk_img_name in set(key[0] for key in similarities.keys()):\n",
    "        # Filter similarities for current NK image\n",
    "        nk_similarities = [(munich_img_name, similarity) for (nk, munich_img_name), similarity in similarities.items() if nk == nk_img_name]\n",
    "        \n",
    "        # Sort by similarity in descending order\n",
    "        sorted_similarities = sorted(nk_similarities, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top 10 similarities\n",
    "        top_10_similarities[nk_img_name] = sorted_similarities[:10]\n",
    "    \n",
    "    return top_10_similarities\n",
    "\n",
    "\n",
    "get_top_10_similarities(sims)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
