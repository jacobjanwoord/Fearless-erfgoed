{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1949_33905_id=cp131507_linz.jpg',\n",
       " '0711_4263_id=cp131504_linz.jpg',\n",
       " '0271_1458_id=cp131506_linz.jpg',\n",
       " '0606_3010_id=cp131502_linz.jpg',\n",
       " '0791_4970_id=cp131503_linz.jpg',\n",
       " '1950_33912_id=cp131509_linz.jpg',\n",
       " '1141_8563_id=cp131500_linz.jpg',\n",
       " '0270_1455_id=cp131505_linz.jpg',\n",
       " '1153_8691_id=cp131501_linz.jpg',\n",
       " '1950_33908_id=cp131508_linz.jpg']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "munich_imgs = os.listdir(\"scraped_images_grayscaled_big\")\n",
    "furnitures = [r for r in munich_imgs if \"linz\" and \"cp13150\" in r ]\n",
    "furnitures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #vgg models (16 and 19 similar, but 16 is better)\n",
    "# # model = torchvision.models.vgg16(pretrained=True)\n",
    "# # model = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "# model.features[0] = nn.Conv2d(1,64,kernel_size=(3,3), stride=(1,1),padding=(1,1))\n",
    "# model = nn.Sequential(*[*list(model.children())[:-1][0][:-10]])\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     This function takes a path to a single image, it then resizes it to size 50x50 \\\n",
    "#     and normalizes it to the range [0,1]. Lastly, it adds an extra dimension to the image \\\n",
    "#     which represents the batch size. These steps are needed, because we want to pass the image \\\n",
    "#     to a CNN. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     img = cv2.imread(image_path, -1)\n",
    "#     clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8,8))\n",
    "#     img = clahe.apply(img)\n",
    "#     _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "#     contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "#     mask = np.ones(img.shape, np.uint8)\n",
    "#     mask.fill(255)\n",
    "#     cv2.drawContours(mask, contours, 0, 0, -1)\n",
    "#     img = cv2.add(thresh, mask)\n",
    "#     kernel = np.ones((5,5), dtype=np.uint8)\n",
    "#     img = cv2.erode(img, kernel, 10)\n",
    "#     img = np.abs(np.max(img) - img)\n",
    "    \n",
    "#     img = cv2.resize(img, (50, 50), interpolation=cv2.INTER_AREA)\n",
    "#     preprocess = torchvision.transforms.Compose([\n",
    "#         torchvision.transforms.ToTensor()\n",
    "#     ])\n",
    "#     img = preprocess(img).unsqueeze(0)\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     This function takes a path to a single image, it then preprocesses the image with the \\\n",
    "#     function preprocess_image. Afterwards it passes the image to the pretrained CNN to extract \\\n",
    "#     a feature descriptor. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     img = preprocess_image(image_path)\n",
    "#     with torch.no_grad():\n",
    "#         features = model(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     This function takes the feature descriptor and normalizes it. This is needed as we want \\\n",
    "#     to compute the dot-product similarity between feature descriptors of different images. \\\n",
    "#     And for similarity it is convenient to have all pixels on the same scale without too \\\n",
    "#     much magnitude differences and this also ensures stability. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     return features / np.linalg.norm(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resnet models (gives high scores regardless)\n",
    "# # model = torchvision.models.resnet50(pretrained=True)\n",
    "# # model = torchvision.models.resnet101(pretrained=True) \n",
    "# # model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "# # model = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     This function takes a path to a single image, it then resizes it to size 50x50 \\\n",
    "#     and normalizes it to the range [0,1]. Lastly, it adds an extra dimension to the image \\\n",
    "#     which represents the batch size. These steps are needed, because we want to pass the image \\\n",
    "#     to a CNN. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     img = cv2.imread(image_path, -1)\n",
    "#     clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8,8))\n",
    "#     img = clahe.apply(img)\n",
    "#     _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "#     contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "#     mask = np.ones(img.shape, np.uint8)\n",
    "#     mask.fill(255)\n",
    "#     cv2.drawContours(mask, contours, 0, 0, -1)\n",
    "#     img = cv2.add(thresh, mask)\n",
    "#     kernel = np.ones((5,5), dtype=np.uint8)\n",
    "#     img = cv2.erode(img, kernel, 10)\n",
    "#     img = np.abs(np.max(img) - img)\n",
    "    \n",
    "#     img = cv2.resize(img, (50, 50), interpolation=cv2.INTER_AREA)\n",
    "#     preprocess = torchvision.transforms.Compose([\n",
    "#         torchvision.transforms.ToTensor()\n",
    "#     ])\n",
    "#     img = preprocess(img).unsqueeze(0)\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     This function takes a path to a single image, it then preprocesses the image with the \\\n",
    "#     function preprocess_image. Afterwards it passes the image to the pretrained CNN to extract \\\n",
    "#     a feature descriptor. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     img = preprocess_image(image_path)\n",
    "#     with torch.no_grad():\n",
    "#         features = model(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     This function takes the feature descriptor and normalizes it. This is needed as we want \\\n",
    "#     to compute the dot-product similarity between feature descriptors of different images. \\\n",
    "#     And for similarity it is convenient to have all pixels on the same scale without too \\\n",
    "#     much magnitude differences and this also ensures stability. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     return features / np.linalg.norm(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jonathan/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# InceptionV3 model (potential)\n",
    "model = torchvision.models.inception_v3(pretrained=True, aux_logits=True)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Preprocesses a single image for InceptionV3 input.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image as grayscale\n",
    "    img = cv2.resize(img, (299, 299))  # Resize to 299x299 for InceptionV3\n",
    "    img = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "    # Since the model expects 3 channels, we stack the grayscale image to create 3 channels\n",
    "    img = np.stack([img, img, img], axis=-1)\n",
    "    \n",
    "    img = (img - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
    "    img = np.transpose(img, (2, 0, 1))  # Transpose to (channels, height, width)\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    img = torch.tensor(img, dtype=torch.float32)  # Convert to PyTorch tensor with dtype float32\n",
    "    return img\n",
    "\n",
    "def extract_features(image_path):\n",
    "    \"\"\"\n",
    "    Extract features from a single image using InceptionV3.\n",
    "    \"\"\"\n",
    "    img = preprocess_image(image_path)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "    return features.squeeze(0).numpy()\n",
    "\n",
    "def normalize_features(features):\n",
    "    \"\"\"\n",
    "    Normalize the extracted features.\n",
    "    \"\"\"\n",
    "    return features / np.linalg.norm(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mobile net v2 (gives to high of scores to everything)\n",
    "# model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# # Modify the first convolutional layer to accept grayscale images\n",
    "# model.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     Preprocesses a single grayscale image for MobileNetV2 input.\n",
    "#     \"\"\"\n",
    "#     img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image as grayscale\n",
    "#     img = cv2.resize(img, (224, 224))  # Resize to 224x224 for MobileNetV2\n",
    "#     img = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "#     img = np.expand_dims(img, axis=0)  # Add channel dimension\n",
    "#     img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "#     img = torch.tensor(img, dtype=torch.float32)  # Convert to PyTorch tensor with dtype float32\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     Extract features from a single image using MobileNetV2.\n",
    "#     \"\"\"\n",
    "#     img = preprocess_image(image_path)\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():\n",
    "#         features = model.features(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     Normalize the extracted features.\n",
    "#     \"\"\"\n",
    "#     return features / np.linalg.norm(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DenseNet (gives similar scores to everything)\n",
    "# model = torchvision.models.densenet121(pretrained=True)\n",
    "\n",
    "# # Modify the first convolutional layer to accept grayscale images\n",
    "# model.features.conv0 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     Preprocesses a single grayscale image for DenseNet input.\n",
    "#     \"\"\"\n",
    "#     img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image as grayscale\n",
    "#     img = cv2.resize(img, (224, 224))  # Resize to 224x224 for DenseNet\n",
    "#     img = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "#     img = np.expand_dims(img, axis=0)  # Add channel dimension\n",
    "#     img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "#     img = torch.tensor(img, dtype=torch.float32)  # Convert to PyTorch tensor with dtype float32\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     Extract features from a single image using DenseNet.\n",
    "#     \"\"\"\n",
    "#     img = preprocess_image(image_path)\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():\n",
    "#         features = model.features(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     Normalize the extracted features.\n",
    "#     \"\"\"\n",
    "#     return features / np.linalg.norm(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alexnet\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "# # Modify the first convolutional layer to accept grayscale images\n",
    "# model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     Preprocesses a single grayscale image for AlexNet input.\n",
    "#     \"\"\"\n",
    "#     img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image as grayscale\n",
    "#     img = cv2.resize(img, (224, 224))  # Resize to 224x224 for AlexNet\n",
    "#     img = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "#     img = np.expand_dims(img, axis=0)  # Add channel dimension\n",
    "#     img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "#     img = torch.tensor(img, dtype=torch.float32)  # Convert to PyTorch tensor with dtype float32\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     Extract features from a single image using AlexNet.\n",
    "#     \"\"\"\n",
    "#     img = preprocess_image(image_path)\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():\n",
    "#         features = model.features(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     Normalize the extracted features.\n",
    "#     \"\"\"\n",
    "#     return features / np.linalg.norm(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BasicConv2d(\n",
       "   (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "   (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       " ),\n",
       " BasicConv2d(\n",
       "   (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "   (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       " ),\n",
       " BasicConv2d(\n",
       "   (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "   (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       " ),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " BasicConv2d(\n",
       "   (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       " ),\n",
       " BasicConv2d(\n",
       "   (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "   (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       " ),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " InceptionA(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch5x5_1): BasicConv2d(\n",
       "     (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch5x5_2): BasicConv2d(\n",
       "     (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionA(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch5x5_1): BasicConv2d(\n",
       "     (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch5x5_2): BasicConv2d(\n",
       "     (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionA(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch5x5_1): BasicConv2d(\n",
       "     (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch5x5_2): BasicConv2d(\n",
       "     (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionB(\n",
       "   (branch3x3): BasicConv2d(\n",
       "     (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionC(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_2): BasicConv2d(\n",
       "     (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_3): BasicConv2d(\n",
       "     (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_4): BasicConv2d(\n",
       "     (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_5): BasicConv2d(\n",
       "     (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionC(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_2): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_3): BasicConv2d(\n",
       "     (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_4): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_5): BasicConv2d(\n",
       "     (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionC(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_2): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_3): BasicConv2d(\n",
       "     (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_4): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_5): BasicConv2d(\n",
       "     (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionC(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_2): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_3): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_4): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_5): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionAux(\n",
       "   (conv0): BasicConv2d(\n",
       "     (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (conv1): BasicConv2d(\n",
       "     (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       " ),\n",
       " InceptionD(\n",
       "   (branch3x3_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_2): BasicConv2d(\n",
       "     (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7x3_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7x3_2): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7x3_3): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7x3_4): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionE(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_1): BasicConv2d(\n",
       "     (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_2a): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_2b): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3a): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3b): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionE(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_1): BasicConv2d(\n",
       "     (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_2a): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_2b): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3a): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3b): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " AdaptiveAvgPool2d(output_size=(1, 1)),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " Linear(in_features=2048, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('speeltafel_nk.png', 'kast_mccp.jpg'): -0.06027171388268471,\n",
       " ('speeltafel_nk.png', 'dressoir_mccp.jpg'): -0.0602254681289196,\n",
       " ('speeltafel_nk.png', 'tafel_mccp.jpg'): 0.3408922553062439,\n",
       " ('speeltafel_nk.png', 'speeltafel_mccp.png'): 0.4637216031551361,\n",
       " ('speeltafel_nk.png', 'stoel_mccp.jpg'): 0.23356740176677704,\n",
       " ('dressoir_nk.jpg', 'kast_mccp.jpg'): 0.3234820067882538,\n",
       " ('dressoir_nk.jpg', 'dressoir_mccp.jpg'): 0.5264071226119995,\n",
       " ('dressoir_nk.jpg', 'tafel_mccp.jpg'): 0.26638662815093994,\n",
       " ('dressoir_nk.jpg', 'speeltafel_mccp.png'): 0.24063123762607574,\n",
       " ('dressoir_nk.jpg', 'stoel_mccp.jpg'): 0.1961786448955536,\n",
       " ('kast_nk.jpg', 'kast_mccp.jpg'): 0.7655638456344604,\n",
       " ('kast_nk.jpg', 'dressoir_mccp.jpg'): 0.3231736421585083,\n",
       " ('kast_nk.jpg', 'tafel_mccp.jpg'): 0.18063046038150787,\n",
       " ('kast_nk.jpg', 'speeltafel_mccp.png'): 0.08670418709516525,\n",
       " ('kast_nk.jpg', 'stoel_mccp.jpg'): 0.07595200091600418,\n",
       " ('tafel_nk.jpg', 'kast_mccp.jpg'): 0.2031152993440628,\n",
       " ('tafel_nk.jpg', 'dressoir_mccp.jpg'): 0.20077355206012726,\n",
       " ('tafel_nk.jpg', 'tafel_mccp.jpg'): 0.5392245054244995,\n",
       " ('tafel_nk.jpg', 'speeltafel_mccp.png'): 0.44978219270706177,\n",
       " ('tafel_nk.jpg', 'stoel_mccp.jpg'): 0.3655223548412323,\n",
       " ('stoel_nk.jpg', 'kast_mccp.jpg'): 0.17944806814193726,\n",
       " ('stoel_nk.jpg', 'dressoir_mccp.jpg'): 0.11186058074235916,\n",
       " ('stoel_nk.jpg', 'tafel_mccp.jpg'): 0.257001668214798,\n",
       " ('stoel_nk.jpg', 'speeltafel_mccp.png'): 0.1668144166469574,\n",
       " ('stoel_nk.jpg', 'stoel_mccp.jpg'): 0.6677655577659607}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nk_testset = os.listdir(\"nk_testset\")\n",
    "munich_testset = os.listdir(\"munich_testset\")\n",
    "\n",
    "def compute_similarities_testsets(munich_testset, nk_testset, \n",
    "                                  munich_path=\"munich_testset\", \n",
    "                                  nk_path=\"nk_testset\"):\n",
    "    \"\"\"\n",
    "    This function takes four arguments: \n",
    "    - munich_testset, which contains 5 grayscaled images from the munich database.\n",
    "    - nk_testset, which contains 5 grayscaled images from the nk collection API.\n",
    "    - munich path, the path to the directory of the munich images. \n",
    "    - nk_path, the path to the directory of the nk images. \n",
    "    \n",
    "    It then computes the feature descriptors for the munich images and all the \\\n",
    "    nk collection images. Afterwards takes the dot-product to get the dot-product similiarity. \n",
    "    It then saves the similarity and the two images as key-value pairs in a dictionary. \n",
    "    \"\"\"\n",
    "    \n",
    "    similarities = {}\n",
    "    for nk_img in nk_testset:\n",
    "        nk_img_path = os.path.join(nk_path, nk_img)\n",
    "        for munich_img in munich_testset:\n",
    "            munich_img_path = os.path.join(munich_path, munich_img)\n",
    "            nk_img_feature_descriptor = normalize_features(extract_features(nk_img_path).flatten())\n",
    "            munich_img_feature_descriptor = normalize_features(extract_features(munich_img_path).flatten())\n",
    "            similarity = np.dot(\n",
    "                nk_img_feature_descriptor,\n",
    "                munich_img_feature_descriptor\n",
    "            )\n",
    "            similarities[(nk_img, munich_img)] = similarity.item()\n",
    "        \n",
    "    return similarities\n",
    "    \n",
    "sims = compute_similarities_testsets(munich_testset, nk_testset)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kast_mccp</th>\n",
       "      <th>dressoir_mccp</th>\n",
       "      <th>tafel_mccp</th>\n",
       "      <th>speeltafel_mccp</th>\n",
       "      <th>stoel_mccp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>speeltafel_nk</th>\n",
       "      <td>-0.060272</td>\n",
       "      <td>-0.060225</td>\n",
       "      <td>0.340892</td>\n",
       "      <td>0.463722</td>\n",
       "      <td>0.233567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dressoir_nk</th>\n",
       "      <td>0.323482</td>\n",
       "      <td>0.526407</td>\n",
       "      <td>0.266387</td>\n",
       "      <td>0.240631</td>\n",
       "      <td>0.196179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kast_nk</th>\n",
       "      <td>0.765564</td>\n",
       "      <td>0.323174</td>\n",
       "      <td>0.180630</td>\n",
       "      <td>0.086704</td>\n",
       "      <td>0.075952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tafel_nk</th>\n",
       "      <td>0.203115</td>\n",
       "      <td>0.200774</td>\n",
       "      <td>0.539225</td>\n",
       "      <td>0.449782</td>\n",
       "      <td>0.365522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoel_nk</th>\n",
       "      <td>0.179448</td>\n",
       "      <td>0.111861</td>\n",
       "      <td>0.257002</td>\n",
       "      <td>0.166814</td>\n",
       "      <td>0.667766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               kast_mccp  dressoir_mccp  tafel_mccp  speeltafel_mccp  \\\n",
       "speeltafel_nk  -0.060272      -0.060225    0.340892         0.463722   \n",
       "dressoir_nk     0.323482       0.526407    0.266387         0.240631   \n",
       "kast_nk         0.765564       0.323174    0.180630         0.086704   \n",
       "tafel_nk        0.203115       0.200774    0.539225         0.449782   \n",
       "stoel_nk        0.179448       0.111861    0.257002         0.166814   \n",
       "\n",
       "               stoel_mccp  \n",
       "speeltafel_nk    0.233567  \n",
       "dressoir_nk      0.196179  \n",
       "kast_nk          0.075952  \n",
       "tafel_nk         0.365522  \n",
       "stoel_nk         0.667766  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_table(sims):\n",
    "    \"\"\"\n",
    "    This function takes the output produced by either the compute_similarities \\ \n",
    "    or compute_similarities_testsets function, and returns a pandas dataframe/table \\\n",
    "    and also saves it in excel.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    rows = []\n",
    "\n",
    "    for key, value in sims.items():\n",
    "        if key[0] not in data:\n",
    "            data[key[0]] = []\n",
    "        if key[1] not in rows:\n",
    "            rows.append(key[1])\n",
    "        data[key[0]].append(value)\n",
    "        \n",
    "    data = {key[:key.rfind(\".\")]:value for key, value in data.items()}\n",
    "    rows = [row[:row.rfind(\".\")] for row in rows]\n",
    "        \n",
    "    df = pd.DataFrame(data, index=rows)\n",
    "    #df.to_excel('output.xlsx')\n",
    "    return df.T\n",
    "    \n",
    "get_table(sims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
