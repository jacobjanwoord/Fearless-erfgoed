{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "base_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1949_33905_id=cp131507_linz.jpg',\n",
       " '0711_4263_id=cp131504_linz.jpg',\n",
       " '0271_1458_id=cp131506_linz.jpg',\n",
       " '0606_3010_id=cp131502_linz.jpg',\n",
       " '0791_4970_id=cp131503_linz.jpg',\n",
       " '1950_33912_id=cp131509_linz.jpg',\n",
       " '1141_8563_id=cp131500_linz.jpg',\n",
       " '0270_1455_id=cp131505_linz.jpg',\n",
       " '1153_8691_id=cp131501_linz.jpg',\n",
       " '1950_33908_id=cp131508_linz.jpg']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "munich_imgs = os.listdir(\"scraped_images_grayscaled_big\")\n",
    "furnitures = [r for r in munich_imgs if \"linz\" and \"cp13150\" in r ]\n",
    "furnitures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jonathan/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# #vgg models (16 and 19 similar, but 16 is better)\n",
    "# model = torchvision.models.vgg16(pretrained=True)\n",
    "# # model = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "# model.features[0] = nn.Conv2d(1,64,kernel_size=(3,3), stride=(1,1),padding=(1,1))\n",
    "# model = nn.Sequential(*[*list(model.children())[:-1][0][:-10]])\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     This function takes a path to a single image, it then resizes it to size 50x50 \\\n",
    "#     and normalizes it to the range [0,1]. Lastly, it adds an extra dimension to the image \\\n",
    "#     which represents the batch size. These steps are needed, because we want to pass the image \\\n",
    "#     to a CNN. \n",
    "#     \"\"\"\n",
    "\n",
    "#     # Read the image in grayscale mode\n",
    "#     img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "#     if img is None:\n",
    "#         raise ValueError(f\"Image at path {image_path} could not be loaded.\")\n",
    "    \n",
    "#     # Apply CLAHE (histogram equalization)\n",
    "#     clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8,8))\n",
    "#     img = clahe.apply(img)\n",
    "\n",
    "#     # Apply thresholding\n",
    "#     _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "#     contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "#     mask = np.ones(img.shape, np.uint8) * 255\n",
    "#     cv2.drawContours(mask, contours, 0, 0, -1)\n",
    "#     img = cv2.add(thresh, mask)\n",
    "\n",
    "#     # Apply erosion\n",
    "#     kernel = np.ones((5,5), dtype=np.uint8)\n",
    "#     img = cv2.erode(img, kernel, 10)\n",
    "#     img = np.abs(np.max(img) - img)\n",
    "\n",
    "#     # Apply gamma correction\n",
    "#     img = np.power(img / 255.0, 2.0) * 255.0\n",
    "#     img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "\n",
    "#     # Resize image to 50x50\n",
    "#     img = cv2.resize(img, (50, 50), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "#     # Convert image to tensor\n",
    "#     preprocess = torchvision.transforms.Compose([\n",
    "#         torchvision.transforms.ToTensor()\n",
    "#     ])\n",
    "#     img = preprocess(img).unsqueeze(0)\n",
    "#     return img\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     This function takes a path to a single image, it then preprocesses the image with the \\\n",
    "#     function preprocess_image. Afterwards it passes the image to the pretrained CNN to extract \\\n",
    "#     a feature descriptor. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     img = preprocess_image(image_path)\n",
    "#     with torch.no_grad():\n",
    "#         features = model(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     This function takes the feature descriptor and normalizes it. This is needed as we want \\\n",
    "#     to compute the dot-product similarity between feature descriptors of different images. \\\n",
    "#     And for similarity it is convenient to have all pixels on the same scale without too \\\n",
    "#     much magnitude differences and this also ensures stability. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     return features / np.linalg.norm(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resnet models (gives high scores regardless)\n",
    "# # model = torchvision.models.resnet50(pretrained=True)\n",
    "# # model = torchvision.models.resnet101(pretrained=True) \n",
    "# # model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "# # model = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     This function takes a path to a single image, it then resizes it to size 50x50 \\\n",
    "#     and normalizes it to the range [0,1]. Lastly, it adds an extra dimension to the image \\\n",
    "#     which represents the batch size. These steps are needed, because we want to pass the image \\\n",
    "#     to a CNN. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     img = cv2.imread(image_path, -1)\n",
    "#     clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8,8))\n",
    "#     img = clahe.apply(img)\n",
    "#     _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "#     contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "#     mask = np.ones(img.shape, np.uint8)\n",
    "#     mask.fill(255)\n",
    "#     cv2.drawContours(mask, contours, 0, 0, -1)\n",
    "#     img = cv2.add(thresh, mask)\n",
    "#     kernel = np.ones((5,5), dtype=np.uint8)\n",
    "#     img = cv2.erode(img, kernel, 10)\n",
    "#     img = np.abs(np.max(img) - img)\n",
    "    \n",
    "#     img = cv2.resize(img, (50, 50), interpolation=cv2.INTER_AREA)\n",
    "#     preprocess = torchvision.transforms.Compose([\n",
    "#         torchvision.transforms.ToTensor()\n",
    "#     ])\n",
    "#     img = preprocess(img).unsqueeze(0)\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     This function takes a path to a single image, it then preprocesses the image with the \\\n",
    "#     function preprocess_image. Afterwards it passes the image to the pretrained CNN to extract \\\n",
    "#     a feature descriptor. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     img = preprocess_image(image_path)\n",
    "#     with torch.no_grad():\n",
    "#         features = model(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     This function takes the feature descriptor and normalizes it. This is needed as we want \\\n",
    "#     to compute the dot-product similarity between feature descriptors of different images. \\\n",
    "#     And for similarity it is convenient to have all pixels on the same scale without too \\\n",
    "#     much magnitude differences and this also ensures stability. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     return features / np.linalg.norm(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# InceptionV3 model (potential)\n",
    "model = torchvision.models.inception_v3(pretrained=True, aux_logits=True)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Preprocesses a single image for InceptionV3 input.\n",
    "    \"\"\"\n",
    "    # img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image as grayscale\n",
    "    # img = cv2.resize(img, (299, 299))  # Resize to 299x299 for InceptionV3\n",
    "    # img = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "    # # Since the model expects 3 channels, we stack the grayscale image to create 3 channels\n",
    "    # img = np.stack([img, img, img], axis=-1)\n",
    "    \n",
    "    # img = (img - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
    "    # img = np.transpose(img, (2, 0, 1))  # Transpose to (channels, height, width)\n",
    "    # img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    # img = torch.tensor(img, dtype=torch.float32)  # Convert to PyTorch tensor with dtype float32\n",
    "        # Read the image as grayscale\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize to 299x299 for InceptionV3\n",
    "    img = cv2.resize(img, (299, 299))\n",
    "    \n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    img = clahe.apply(img)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    img = np.array(img, dtype=np.float32) / 255.0\n",
    "    \n",
    "    # Since the model expects 3 channels, we stack the grayscale image to create 3 channels\n",
    "    img = np.stack([img, img, img], axis=-1)\n",
    "    \n",
    "    # Apply Gamma correction\n",
    "    gamma = 1.2  # Example gamma value, adjust as necessary\n",
    "    img = np.power(img, gamma)\n",
    "    \n",
    "    # Normalize with ImageNet mean and std\n",
    "    img = (img - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    # Transpose to (channels, height, width)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    # Convert to PyTorch tensor with dtype float32\n",
    "    img = torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "    return img\n",
    "\n",
    "def extract_features(image_path):\n",
    "    \"\"\"\n",
    "    Extract features from a single image using InceptionV3.\n",
    "    \"\"\"\n",
    "    img = preprocess_image(image_path)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "    return features.squeeze(0).numpy()\n",
    "\n",
    "def normalize_features(features):\n",
    "    \"\"\"\n",
    "    Normalize the extracted features.\n",
    "    \"\"\"\n",
    "    return features / np.linalg.norm(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mobile net v2 (gives to high of scores to everything)\n",
    "# model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# # Modify the first convolutional layer to accept grayscale images\n",
    "# model.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     Preprocesses a single grayscale image for MobileNetV2 input.\n",
    "#     \"\"\"\n",
    "#     img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image as grayscale\n",
    "#     img = cv2.resize(img, (224, 224))  # Resize to 224x224 for MobileNetV2\n",
    "#     img = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "#     img = np.expand_dims(img, axis=0)  # Add channel dimension\n",
    "#     img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "#     img = torch.tensor(img, dtype=torch.float32)  # Convert to PyTorch tensor with dtype float32\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     Extract features from a single image using MobileNetV2.\n",
    "#     \"\"\"\n",
    "#     img = preprocess_image(image_path)\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():\n",
    "#         features = model.features(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     Normalize the extracted features.\n",
    "#     \"\"\"\n",
    "#     return features / np.linalg.norm(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DenseNet (gives similar scores to everything)\n",
    "# model = torchvision.models.densenet121(pretrained=True)\n",
    "\n",
    "# # Modify the first convolutional layer to accept grayscale images\n",
    "# model.features.conv0 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     Preprocesses a single grayscale image for DenseNet input.\n",
    "#     \"\"\"\n",
    "#     img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image as grayscale\n",
    "#     img = cv2.resize(img, (224, 224))  # Resize to 224x224 for DenseNet\n",
    "#     img = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "#     img = np.expand_dims(img, axis=0)  # Add channel dimension\n",
    "#     img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "#     img = torch.tensor(img, dtype=torch.float32)  # Convert to PyTorch tensor with dtype float32\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     Extract features from a single image using DenseNet.\n",
    "#     \"\"\"\n",
    "#     img = preprocess_image(image_path)\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():\n",
    "#         features = model.features(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     Normalize the extracted features.\n",
    "#     \"\"\"\n",
    "#     return features / np.linalg.norm(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alexnet (gives good scores unconditionally)\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "# # Modify the first convolutional layer to accept grayscale images\n",
    "# model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"\n",
    "#     Preprocesses a single grayscale image for AlexNet input.\n",
    "#     \"\"\"\n",
    "#     img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image as grayscale\n",
    "#     img = cv2.resize(img, (224, 224))  # Resize to 224x224 for AlexNet\n",
    "#     img = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "#     img = np.expand_dims(img, axis=0)  # Add channel dimension\n",
    "#     img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "#     img = torch.tensor(img, dtype=torch.float32)  # Convert to PyTorch tensor with dtype float32\n",
    "#     return img\n",
    "\n",
    "# def extract_features(image_path):\n",
    "#     \"\"\"\n",
    "#     Extract features from a single image using AlexNet.\n",
    "#     \"\"\"\n",
    "#     img = preprocess_image(image_path)\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():\n",
    "#         features = model.features(img)\n",
    "#     return features.squeeze(0).numpy()\n",
    "\n",
    "# def normalize_features(features):\n",
    "#     \"\"\"\n",
    "#     Normalize the extracted features.\n",
    "#     \"\"\"\n",
    "#     return features / np.linalg.norm(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BasicConv2d(\n",
       "   (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "   (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       " ),\n",
       " BasicConv2d(\n",
       "   (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "   (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       " ),\n",
       " BasicConv2d(\n",
       "   (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "   (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       " ),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " BasicConv2d(\n",
       "   (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       " ),\n",
       " BasicConv2d(\n",
       "   (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "   (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       " ),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " InceptionA(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch5x5_1): BasicConv2d(\n",
       "     (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch5x5_2): BasicConv2d(\n",
       "     (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionA(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch5x5_1): BasicConv2d(\n",
       "     (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch5x5_2): BasicConv2d(\n",
       "     (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionA(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch5x5_1): BasicConv2d(\n",
       "     (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch5x5_2): BasicConv2d(\n",
       "     (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionB(\n",
       "   (branch3x3): BasicConv2d(\n",
       "     (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionC(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_2): BasicConv2d(\n",
       "     (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_3): BasicConv2d(\n",
       "     (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_4): BasicConv2d(\n",
       "     (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_5): BasicConv2d(\n",
       "     (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionC(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_2): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_3): BasicConv2d(\n",
       "     (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_4): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_5): BasicConv2d(\n",
       "     (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionC(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_2): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_3): BasicConv2d(\n",
       "     (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_4): BasicConv2d(\n",
       "     (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_5): BasicConv2d(\n",
       "     (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionC(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_2): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7_3): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_3): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_4): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7dbl_5): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionAux(\n",
       "   (conv0): BasicConv2d(\n",
       "     (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (conv1): BasicConv2d(\n",
       "     (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       " ),\n",
       " InceptionD(\n",
       "   (branch3x3_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_2): BasicConv2d(\n",
       "     (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7x3_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7x3_2): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7x3_3): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch7x7x3_4): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionE(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_1): BasicConv2d(\n",
       "     (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_2a): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_2b): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3a): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3b): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " InceptionE(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_1): BasicConv2d(\n",
       "     (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_2a): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3_2b): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3a): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch3x3dbl_3b): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " AdaptiveAvgPool2d(output_size=(1, 1)),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " Linear(in_features=2048, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_model_state_dict = model.state_dict()\n",
    "# torch.save(best_model_state_dict, \"best_inception_v3_weights.pth\")\n",
    "best_model_state_dict = torch.load(\"best2_vgg16_weights.pth\")\n",
    "model.load_state_dict(best_model_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities_testsets(munich_testset, nk_testset, \n",
    "                                  munich_path=\"munich_test_no_back\", \n",
    "                                  nk_path=\"nk_test_no_back\"):\n",
    "    \"\"\"\n",
    "    This function takes four arguments: \n",
    "    - munich_testset, which contains 5 grayscaled images from the munich database.\n",
    "    - nk_testset, which contains 5 grayscaled images from the nk collection API.\n",
    "    - munich path, the path to the directory of the munich images. \n",
    "    - nk_path, the path to the directory of the nk images. \n",
    "    \n",
    "    It then computes the feature descriptors for the munich images and all the \\\n",
    "    nk collection images. Afterwards takes the dot-product to get the dot-product similiarity. \n",
    "    It then saves the similarity and the two images as key-value pairs in a dictionary. \n",
    "    \"\"\"\n",
    "    \n",
    "    similarities = {}\n",
    "    for nk_img in nk_testset:\n",
    "        nk_img_path = os.path.join(nk_path, nk_img)\n",
    "        for munich_img in munich_testset:\n",
    "            munich_img_path = os.path.join(munich_path, munich_img)\n",
    "            nk_img_feature_descriptor = normalize_features(extract_features(nk_img_path).flatten())\n",
    "            munich_img_feature_descriptor = normalize_features(extract_features(munich_img_path).flatten())\n",
    "            similarity = np.dot(\n",
    "                nk_img_feature_descriptor,\n",
    "                munich_img_feature_descriptor\n",
    "            )\n",
    "            similarities[(nk_img, munich_img)] = similarity.item()\n",
    "        \n",
    "    return similarities\n",
    "\n",
    "def compute_similarities_img_to_set(nk_img, munich_testset, \n",
    "                                    munich_path=\"mc_no_back\", \n",
    "                                    nk_path=\"nk_no_back\"):\n",
    "    similarities = {}\n",
    "    nk_img_path = os.path.join(nk_path, nk_img)\n",
    "    \n",
    "    # Ensure the NK image file exists\n",
    "    if not os.path.exists(nk_img_path):\n",
    "        raise FileNotFoundError(f\"NK image file {nk_img_path} not found.\")\n",
    "    \n",
    "    nk_img_feature_descriptor = normalize_features(extract_features(nk_img_path).flatten())\n",
    "    \n",
    "    for munich_img in munich_testset:\n",
    "        munich_img_path = os.path.join(munich_path, munich_img)\n",
    "        \n",
    "        # Ensure the Munich image file exists\n",
    "        if not os.path.exists(munich_img_path):\n",
    "            raise FileNotFoundError(f\"Munich image file {munich_img_path} not found.\")\n",
    "        \n",
    "        munich_img_feature_descriptor = normalize_features(extract_features(munich_img_path).flatten())\n",
    "        similarity = np.dot(\n",
    "            nk_img_feature_descriptor,\n",
    "            munich_img_feature_descriptor\n",
    "        )\n",
    "        similarities[(nk_img, munich_img)] = similarity.item()\n",
    "        \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('speeltafel_nk.png', 'kast_mccp.jpg'): 0.04646206274628639,\n",
       " ('speeltafel_nk.png', 'dressoir_mccp.jpg'): 0.04062419757246971,\n",
       " ('speeltafel_nk.png', 'tafel_mccp.jpg'): 0.2207023650407791,\n",
       " ('speeltafel_nk.png', 'speeltafel_mccp.png'): 0.4970906674861908,\n",
       " ('speeltafel_nk.png', 'stoel_mccp.jpg'): 0.11298052966594696,\n",
       " ('dressoir_nk.jpg', 'kast_mccp.jpg'): 0.45805615186691284,\n",
       " ('dressoir_nk.jpg', 'dressoir_mccp.jpg'): 0.4778445065021515,\n",
       " ('dressoir_nk.jpg', 'tafel_mccp.jpg'): 0.35593485832214355,\n",
       " ('dressoir_nk.jpg', 'speeltafel_mccp.png'): 0.08322122693061829,\n",
       " ('dressoir_nk.jpg', 'stoel_mccp.jpg'): 0.21342997252941132,\n",
       " ('kast_nk.jpg', 'kast_mccp.jpg'): 0.6803843975067139,\n",
       " ('kast_nk.jpg', 'dressoir_mccp.jpg'): 0.29671618342399597,\n",
       " ('kast_nk.jpg', 'tafel_mccp.jpg'): 0.1898735910654068,\n",
       " ('kast_nk.jpg', 'speeltafel_mccp.png'): 0.040228769183158875,\n",
       " ('kast_nk.jpg', 'stoel_mccp.jpg'): 0.04464560002088547,\n",
       " ('tafel_nk.jpg', 'kast_mccp.jpg'): 0.2868760824203491,\n",
       " ('tafel_nk.jpg', 'dressoir_mccp.jpg'): 0.215520977973938,\n",
       " ('tafel_nk.jpg', 'tafel_mccp.jpg'): 0.46839654445648193,\n",
       " ('tafel_nk.jpg', 'speeltafel_mccp.png'): 0.20008303225040436,\n",
       " ('tafel_nk.jpg', 'stoel_mccp.jpg'): 0.43395206332206726,\n",
       " ('stoel_nk.jpg', 'kast_mccp.jpg'): 0.230801060795784,\n",
       " ('stoel_nk.jpg', 'dressoir_mccp.jpg'): 0.13051119446754456,\n",
       " ('stoel_nk.jpg', 'tafel_mccp.jpg'): 0.4917014241218567,\n",
       " ('stoel_nk.jpg', 'speeltafel_mccp.png'): 0.16480641067028046,\n",
       " ('stoel_nk.jpg', 'stoel_mccp.jpg'): 0.6619942784309387}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nk_testset = os.listdir(\"nk_test_no_back\")\n",
    "munich_testset = os.listdir(\"munich_test_no_back\")\n",
    "\n",
    "sims_test = compute_similarities_testsets(munich_testset, nk_testset)\n",
    "sims_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "munich_set = os.listdir(\"scraped_images_grayscaled_big\")\n",
    "nk_img = (\"schilderij_656.jpg\")\n",
    "\n",
    "sims = compute_similarities_img_to_set(nk_img, munich_set, munich_path='scraped_images_grayscaled_big' ,nk_path='nk_collection_schilderij_cleaned')\n",
    "# print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kast_mccp</th>\n",
       "      <th>dressoir_mccp</th>\n",
       "      <th>tafel_mccp</th>\n",
       "      <th>speeltafel_mccp</th>\n",
       "      <th>stoel_mccp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>speeltafel_nk</th>\n",
       "      <td>0.046462</td>\n",
       "      <td>0.040624</td>\n",
       "      <td>0.220702</td>\n",
       "      <td>0.497091</td>\n",
       "      <td>0.112981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dressoir_nk</th>\n",
       "      <td>0.458056</td>\n",
       "      <td>0.477845</td>\n",
       "      <td>0.355935</td>\n",
       "      <td>0.083221</td>\n",
       "      <td>0.213430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kast_nk</th>\n",
       "      <td>0.680384</td>\n",
       "      <td>0.296716</td>\n",
       "      <td>0.189874</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.044646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tafel_nk</th>\n",
       "      <td>0.286876</td>\n",
       "      <td>0.215521</td>\n",
       "      <td>0.468397</td>\n",
       "      <td>0.200083</td>\n",
       "      <td>0.433952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoel_nk</th>\n",
       "      <td>0.230801</td>\n",
       "      <td>0.130511</td>\n",
       "      <td>0.491701</td>\n",
       "      <td>0.164806</td>\n",
       "      <td>0.661994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               kast_mccp  dressoir_mccp  tafel_mccp  speeltafel_mccp  \\\n",
       "speeltafel_nk   0.046462       0.040624    0.220702         0.497091   \n",
       "dressoir_nk     0.458056       0.477845    0.355935         0.083221   \n",
       "kast_nk         0.680384       0.296716    0.189874         0.040229   \n",
       "tafel_nk        0.286876       0.215521    0.468397         0.200083   \n",
       "stoel_nk        0.230801       0.130511    0.491701         0.164806   \n",
       "\n",
       "               stoel_mccp  \n",
       "speeltafel_nk    0.112981  \n",
       "dressoir_nk      0.213430  \n",
       "kast_nk          0.044646  \n",
       "tafel_nk         0.433952  \n",
       "stoel_nk         0.661994  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_table(sims):\n",
    "    \"\"\"\n",
    "    This function takes the output produced by either the compute_similarities \\ \n",
    "    or compute_similarities_testsets function, and returns a pandas dataframe/table \\\n",
    "    and also saves it in excel.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    rows = []\n",
    "\n",
    "    for key, value in sims.items():\n",
    "        if key[0] not in data:\n",
    "            data[key[0]] = []\n",
    "        if key[1] not in rows:\n",
    "            rows.append(key[1])\n",
    "        data[key[0]].append(value)\n",
    "        \n",
    "    data = {key[:key.rfind(\".\")]:value for key, value in data.items()}\n",
    "    rows = [row[:row.rfind(\".\")] for row in rows]\n",
    "        \n",
    "    df = pd.DataFrame(data, index=rows)\n",
    "    #df.to_excel('output.xlsx')\n",
    "    return df.T\n",
    "    \n",
    "get_table(sims_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'schilderij_656.jpg': [('1258_9695_id=cp133005_linz.jpg', 0.8316123485565186),\n",
       "  ('1142_8573_id=cp133252_linz.jpg', 0.7322022914886475),\n",
       "  ('1167_8854_id=cp133488_linz.jpg', 0.7194464206695557),\n",
       "  ('1274_9896_id=cp133853_linz.jpg', 0.6987143158912659),\n",
       "  ('1208_9251_id=cp130334_linz.jpg', 0.6773865222930908),\n",
       "  ('1149_8673_id=cp133832_linz.jpg', 0.6762235164642334),\n",
       "  ('1227_9437_id=cp130649_linz.jpg', 0.6677848696708679),\n",
       "  ('0460_2354-2_id=cp131030_linz.jpg', 0.6628702282905579),\n",
       "  ('0644_3869_id=cp131974_linz.jpg', 0.6587140560150146),\n",
       "  ('0641_3824_id=cp130624_linz.jpg', 0.6557547450065613),\n",
       "  ('1271_9867_id=cp131364_linz.jpg', 0.6462880969047546),\n",
       "  ('1324_10770_id=cp131446_linz.jpg', 0.6325349807739258),\n",
       "  ('1264_9769_id=cp132813_linz.jpg', 0.6267836689949036),\n",
       "  ('0425_2277-1_id=cp131080_linz.jpg', 0.6239756345748901),\n",
       "  ('2354_48760_id=cp133580_linz.jpg', 0.6231393218040466),\n",
       "  ('1271_9868_id=cp177718_badv.jpg', 0.6186420321464539),\n",
       "  ('1271_9868_id=cp133058_linz.jpg', 0.6179033517837524),\n",
       "  ('1206_9238_id=cp130142_linz.jpg', 0.6169726252555847),\n",
       "  ('1343_11094_id=cp131145_linz.jpg', 0.6149424910545349),\n",
       "  ('0220_1154_id=cp133999_linz.jpg', 0.6144818067550659),\n",
       "  ('2387_K 1843_id=cp133038_linz.jpg', 0.6119140386581421),\n",
       "  ('0656_4030_id=cp131326_linz.jpg', 0.6109591126441956),\n",
       "  ('1344_11101_id=cp131386_linz.jpg', 0.6087657809257507),\n",
       "  ('1353_11211_id=cp133854_linz.jpg', 0.6077397465705872),\n",
       "  ('0998_7748_id=cp133423_linz.jpg', 0.6045854687690735),\n",
       "  ('0786_4926_id=cp131805_linz.jpg', 0.6033380031585693),\n",
       "  ('1135_8421_id=cp133311_linz.jpg', 0.6012632846832275),\n",
       "  ('1271_9869_id=cp130078_linz.jpg', 0.6007184386253357),\n",
       "  ('1506_13101_id=cp139688_badv.jpg', 0.599730372428894),\n",
       "  ('0258_1399_id=cp133991_linz.jpg', 0.5983335375785828),\n",
       "  ('1231_9475_id=cp130551_linz.jpg', 0.597517192363739),\n",
       "  ('0877_5819_id=cp169180_badv.jpg', 0.5974923372268677),\n",
       "  ('1190_9076_id=cp131044_linz.jpg', 0.5940629243850708),\n",
       "  ('0996_7725_id=cp133283_linz.jpg', 0.5939499735832214),\n",
       "  ('2353_48709_id=cp133770_linz.jpg', 0.5935101509094238),\n",
       "  ('0552_2632_id=cp131382_linz.jpg', 0.5900564193725586),\n",
       "  ('0798_5015_id=cp167271_badv.jpg', 0.5899165272712708),\n",
       "  ('0460_2354-2_id=cp149709_badv.jpg', 0.5873183608055115),\n",
       "  ('1142_8571_id=cp133407_linz.jpg', 0.5869341492652893),\n",
       "  ('1141_8568_id=cp133247_linz.jpg', 0.5866861939430237),\n",
       "  ('1793_23401_id=cp149653_badv.jpg', 0.5854233503341675),\n",
       "  ('1192_9099_id=cp130546_linz.jpg', 0.584632158279419),\n",
       "  ('0716_4304_id=cp131164_linz.jpg', 0.5839061141014099),\n",
       "  ('2141_43623-1_id=cp161760_badv.jpg', 0.5835921764373779),\n",
       "  ('1267_9816_id=cp130708_linz.jpg', 0.5785646438598633),\n",
       "  ('1272_9872_id=cp130139_linz.jpg', 0.5765888094902039),\n",
       "  ('1439_11722_id=cp131310_linz.jpg', 0.575999915599823),\n",
       "  ('1793_23401_id=cp133623_linz.jpg', 0.5741978287696838),\n",
       "  ('0991_7643_id=cp133346_linz.jpg', 0.572418212890625),\n",
       "  ('1651_18051_id=cp133813_linz.jpg', 0.5720103979110718)]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_10_similarities(similarities):\n",
    "    \"\"\"\n",
    "    Get the top 10 highest similarity values for each NK image from the similarities dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - similarities: A dictionary where keys are (NK image name, Munich image name) tuples\n",
    "                    and values are similarity scores.\n",
    "                    \n",
    "    Returns:\n",
    "    - A dictionary where keys are NK image names and values are lists of tuples\n",
    "      (Munich image name, similarity) sorted by similarity in descending order.\n",
    "    \"\"\"\n",
    "    top_10_similarities = {}\n",
    "    \n",
    "    for nk_img_name in set(key[0] for key in similarities.keys()):\n",
    "        # Filter similarities for current NK image\n",
    "        nk_similarities = [(munich_img_name, similarity) for (nk, munich_img_name), similarity in similarities.items() if nk == nk_img_name]\n",
    "        \n",
    "        # Sort by similarity in descending order\n",
    "        sorted_similarities = sorted(nk_similarities, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top 10 similarities\n",
    "        top_10_similarities[nk_img_name] = sorted_similarities[:50]\n",
    "    \n",
    "    return top_10_similarities\n",
    "\n",
    "\n",
    "get_top_10_similarities(sims)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
