{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file all the code can be found to scrape the images off the munich website, also the grayscaling preprocessing part is added below.\n",
    "\n",
    "Formatting of resulting file names is as follows:\n",
    "\n",
    "0062_327_id=cp154815_badv.jpg (random munich image)\n",
    "\n",
    "the first 4 digit number (0062) means on which page the image can be found on the munich website.\\\n",
    "Second number (327) is very important, is the Munich Number linked to the image\\\n",
    "id=cp154815, is the ID of the image as stored on the munich website\\\n",
    "and the last value (badv) is which folder it is from on the munich website\n",
    "\n",
    "> This naming is used to link the images with the names and able to find the corresponding image back, the munich number is here the most important value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is our way of scraping, we go to the munich website with the cookie needed. With the python package beautifulsoup we can easily find all image tags on the website. The program also looks through the html file to find all the matching munich numbers to add to the name of the munich image. These all get scraped automatically by moving to the next page (as only 20 images are displayed on a page at a time).\n",
    "\n",
    "All images get downloaded and put in the scraped_images folder, which later still gets grayscaled and small images removed into the scraped_images_grayscaled_big folder.\n",
    "\n",
    "Takes a few hours to run as it is downloading 47000 images from 2388 different html pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Amount of pages on the munich website\n",
    "AMOUNT_OF_PAGES = 2388\n",
    "\n",
    "# The needed cookie to be able to download the images\n",
    "cookies = {'PHPSESSID': 'u6h1scn6knqdj77di40e2n6uo3'}\n",
    "\n",
    "all_sizes = []\n",
    "\n",
    "# Directory to save images to\n",
    "img_dir = \"scraped_images\"\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)\n",
    "\n",
    "# Filter out all keinbild images.\n",
    "keinbild_path = \"keinbild.jpg\"\n",
    "keinbild_image = io.imread(keinbild_path)\n",
    "\n",
    "def kein_bild_test(image1, image2):\n",
    "    if image1.shape != image2.shape:\n",
    "        return False\n",
    "\n",
    "\n",
    "    # See if it is similar to the keinbild image\n",
    "    similarity_index = ssim(image1, image2, multichannel=True)\n",
    "    threshold = 0.9\n",
    "\n",
    "    # Returns true if image is a keinbild image and it will not get added to the dataset.\n",
    "    return similarity_index > threshold\n",
    "try:\n",
    "    for i in range(0, AMOUNT_OF_PAGES):\n",
    "        # keep track of progess\n",
    "        print(i)\n",
    "        # URL of the webpage to scrape automatically goes to the next page\n",
    "        url = f\"https://www.dhm.de/datenbank/ccp/dhm_ccp.php?seite=8&current={i * 20}\"\n",
    "        print(url)\n",
    "\n",
    "        try:\n",
    "            # Send a GET request to the webpage with cookies\n",
    "            response = requests.get(url, cookies=cookies, timeout=10)\n",
    "            response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find all image tags\n",
    "            img_tags = soup.find_all('img')\n",
    "\n",
    "            tables = soup.find_all('table', class_='karteikarte')\n",
    "\n",
    "            index = 0\n",
    "            for img in img_tags:\n",
    "                img_url = img.get('src')\n",
    "                \n",
    "                # Handles multiple images when there are more than 1 image in the munich field\n",
    "                if img_url and 'displayimg' in img_url:\n",
    "                    find_id = img_url.find('id=')\n",
    "                    # If there are multiple images linked to one munich number our indexing breaks. This checks for this scenario.\n",
    "                    if img_url[find_id + 12] != '0':\n",
    "                        index -= 1\n",
    "\n",
    "                    # Find the munich number and add it to the image name to find it easier later on.\n",
    "                    munich_no = tables[index].find_all('td', class_='value')[0].get_text(strip=True)\n",
    "                    munich_no = munich_no.replace('/', '-')\n",
    "\n",
    "                    index += 1\n",
    "\n",
    "                    if munich_no != '-':\n",
    "                        # Construct full URL\n",
    "\n",
    "                        img_url = urljoin(url, img_url)\n",
    "\n",
    "                        try:\n",
    "                            # Get the image content with cookies\n",
    "                            img_response = requests.get(img_url, cookies=cookies, timeout=10)\n",
    "                            img_response.raise_for_status()\n",
    "\n",
    "                            # Read the image as an array\n",
    "                            img_array = io.imread(BytesIO(img_response.content))\n",
    "\n",
    "                            if len(img_array.shape) == 2:\n",
    "                                height, width = img_array.shape\n",
    "                            else:\n",
    "                                height, width, _ =  img_array.shape\n",
    "                            all_sizes.append((height,width))\n",
    "                            \n",
    "                            # Checks if image is kein bild\n",
    "                            if not kein_bild_test(img_array, keinbild_image):\n",
    "\n",
    "                                # Extract image filename\n",
    "                                find_id = img_url.find('id=')\n",
    "                                img_basename = img_url[find_id: find_id + 11]\n",
    "\n",
    "                                find_folder = img_url.find('folder=')\n",
    "                                img_basename = img_basename + '_' + img_url[find_folder + 7:]\n",
    "                                \n",
    "                                # Start the image name with what page it can be found on the munich database\n",
    "                                # FORMAT:\n",
    "                                # PAGE_MUNICHNO_BASENAME\n",
    "                                starting_zeros = 4 - len(str(i))\n",
    "                                img_name = '0' * starting_zeros + f\"{i+1}_{munich_no}_{img_basename}\"\n",
    "\n",
    "                                # Ensure the file has a .jpg extension\n",
    "                                if not img_name.endswith('.jpg'):\n",
    "                                    img_name += '.jpg'\n",
    "\n",
    "                                img_path = os.path.join(img_dir, img_name)\n",
    "\n",
    "                                # Save the image\n",
    "                                with open(img_path, 'wb') as img_file:\n",
    "                                    img_file.write(img_response.content)\n",
    "\n",
    "                        except requests.exceptions.RequestException as img_err:\n",
    "                            print(f\"Failed to download image {img_url}: {img_err}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "# Can stop the program with ctrl+C\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Script interrupted by user. Exiting...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grayscales all images in scraped images and creates grayscaled directory also removes small images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grayscales all images in scraped images and creates grayscaled directory also removes small images\n",
    "def gray_scale_large(curr_path=\"scraped_images\", new_path=\"scraped_images_grayscaled_big\"):\n",
    "    # get image directory\n",
    "    imgs = os.listdir(curr_path)\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "\n",
    "    # loop over all images in directory, grayscales and removes small images. then stores them \n",
    "    for img_name in imgs:\n",
    "        old_directory = os.path.join(curr_path, img_name)\n",
    "        new_directory = os.path.join(new_path, img_name)\n",
    "        img = cv2.imread(old_directory)\n",
    "\n",
    "        # grayscale image\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # only store image if it is bigger than 50x50\n",
    "        if img.size > 2500:\n",
    "            cv2.imwrite(new_directory, img)\n",
    "\n",
    "gray_scale_large()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
