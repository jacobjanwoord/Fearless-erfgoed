{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653a9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2eff8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_images(association_obj):\n",
    "    \"\"\"\n",
    "    This function iteratively scrapes 100 records from the API. Each record consists of a variery of \\\n",
    "    child elements, but we chose to only scrape the association name (object type) and the URL to the \\\n",
    "    image, because we only needed the furnitures as requested by the client. \n",
    "    \"\"\"\n",
    "    \n",
    "    img_prefix = \"https://images.memorix.nl/rce/thumb/1600x1600/\"\n",
    "    img_postfix = \".jpg\"\n",
    "    base_url = \"https://rcerijswijk.adlibhosting.com/api.wo2/wwwopac.ashx?database=collect\"\n",
    "    search_param = \"&search=all\"\n",
    "    limit = 100\n",
    "    startfrom = 0\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    while True:\n",
    "        url = f\"{base_url}{search_param}&limit={limit}&startfrom={startfrom}\"\n",
    "        response = requests.get(url)\n",
    "        root = ET.fromstring(response.content)\n",
    "\n",
    "        records = root.findall(\".//record\")\n",
    "        if not records:\n",
    "            break\n",
    "            \n",
    "        for record in records:\n",
    "            associations = [association.text for association in record.findall(\n",
    "            \".//Associated_subject/association.subject\")]\n",
    "            if association_obj in associations:\n",
    "                uuids = [reproduction_ref.text for reproduction_ref in record.findall(\n",
    "                \".//Reproduction/reproduction.reference\")]\n",
    "                association_name = association_obj\n",
    "                for uuid in uuids:\n",
    "                    if uuid:\n",
    "                        img_url = img_prefix + uuid + img_postfix\n",
    "                        all_data.append({\n",
    "                            \"Association\": association_name,\n",
    "                            \"Url\": img_url\n",
    "                        })\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        startfrom += limit\n",
    "    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "    \n",
    "images = select_images(\"meubel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "670514d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(df, directory_name=\"nk_collection_meubels\"):\n",
    "    \"\"\"\n",
    "    This function takes a dataframe produced by the function select_images, and a directory name. \\\n",
    "    Then it creates a directory with the given name if it does not exist, and it downloades all the \\\n",
    "    images in the dataframe and puts it in the directory. \n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(directory_name):\n",
    "        os.makedirs(directory_name)\n",
    "        \n",
    "    for idx, row in df.iterrows():\n",
    "        img_url = row[\"Url\"]\n",
    "        img_name = os.path.join(directory_name, f\"meubel_{idx+1}.jpg\")\n",
    "        response = requests.get(img_url, stream=True)\n",
    "        with open(img_name, \"wb\") as file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                file.write(chunk)\n",
    "        \n",
    "download_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "56e29d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray_scale(curr_path=\"nk_collection_meubels\", new_path=\"nk_collection_meubels_cleaned\"):\n",
    "    \"\"\"\n",
    "    This function takes a path to the images that are not yet gray scaled, called curr_path. It also \\\n",
    "    takes a path to the directory that we are going to create to put the gray scaled images in, called \\\n",
    "    new_path. It then iteratively gray scales each image and puts it in the new directory. \n",
    "    \"\"\"\n",
    "    \n",
    "    imgs = os.listdir(curr_path)\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "        \n",
    "    for img_name in imgs:\n",
    "        old_directory = os.path.join(curr_path, img_name)\n",
    "        new_directory = os.path.join(new_path, img_name)\n",
    "        img = cv2.imread(old_directory)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        cv2.imwrite(new_directory, img)\n",
    "\n",
    "gray_scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ce9e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1153_8691_id=cp131501_linz.jpg',\n",
       " '0270_1455_id=cp131505_linz.jpg',\n",
       " '1950_33908_id=cp131508_linz.jpg',\n",
       " '0606_3010_id=cp131502_linz.jpg',\n",
       " '1949_33905_id=cp131507_linz.jpg',\n",
       " '0791_4970_id=cp131503_linz.jpg',\n",
       " '0271_1458_id=cp131506_linz.jpg',\n",
       " '1950_33912_id=cp131509_linz.jpg',\n",
       " '1141_8563_id=cp131500_linz.jpg',\n",
       " '0711_4263_id=cp131504_linz.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "munich_imgs = os.listdir(\"/home/hamid/Downloads/scraped_images_grayscaled_big\")\n",
    "furnitures = [r for r in munich_imgs if \"linz\" and \"cp13150\" in r ]\n",
    "furnitures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1afd7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "model.features[0] = nn.Conv2d(1,64,kernel_size=(3,3), stride=(1,1),padding=(1,1))\n",
    "model = nn.Sequential(*[*list(model.children())[:-1][0][:-10]])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    This function takes a path to a single image, it then resizes it to size 50x50 \\\n",
    "    and normalizes it to the range [0,1]. Lastly, it adds an extra dimension to the image \\\n",
    "    which represents the batch size. These steps are needed, because we want to pass the image \\\n",
    "    to a CNN. \n",
    "    \"\"\"\n",
    "    \n",
    "    img = cv2.imread(image_path, -1)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8,8))\n",
    "    img = clahe.apply(img)\n",
    "    _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    mask = np.ones(img.shape, np.uint8)\n",
    "    mask.fill(255)\n",
    "    cv2.drawContours(mask, contours, 0, 0, -1)\n",
    "    img = cv2.add(thresh, mask)\n",
    "    kernel = np.ones((5,5), dtype=np.uint8)\n",
    "    img = cv2.erode(img, kernel, 10)\n",
    "    img = np.abs(np.max(img) - img)\n",
    "    \n",
    "    img = cv2.resize(img, (50, 50), interpolation=cv2.INTER_AREA)\n",
    "    preprocess = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    img = preprocess(img).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "def extract_features(image_path):\n",
    "    \"\"\"\n",
    "    This function takes a path to a single image, it then preprocesses the image with the \\\n",
    "    function preprocess_image. Afterwards it passes the image to the pretrained CNN to extract \\\n",
    "    a feature descriptor. \n",
    "    \"\"\"\n",
    "    \n",
    "    img = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "    return features.squeeze(0).numpy()\n",
    "\n",
    "def normalize_features(features):\n",
    "    \"\"\"\n",
    "    This function takes the feature descriptor and normalizes it. This is needed as we want \\\n",
    "    to compute the dot-product similarity between feature descriptors of different images. \\\n",
    "    And for similarity it is convenient to have all pixels on the same scale without too \\\n",
    "    much magnitude differences and this also ensures stability. \n",
    "    \"\"\"\n",
    "    \n",
    "    return features / np.linalg.norm(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce9fdc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b380a8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5098/562390912.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mcompute_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmunich_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnk_furnitures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5098/562390912.py\u001b[0m in \u001b[0;36mcompute_similarities\u001b[0;34m(munich_img, nk_collection, path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnk_collection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mnk_img_feature_descriptor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mmunich_img_feature_descriptor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmunich_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         similarity = np.dot(\n",
      "\u001b[0;32m/tmp/ipykernel_5098/2208332052.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5098/2208332052.py\u001b[0m in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mclahe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateCLAHE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipLimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtileGridSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclahe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m127\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_BINARY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcontours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhierarchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindContours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETR_TREE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHAIN_APPROX_SIMPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "munich_img = \"/home/hamid/Downloads/scraped_images_grayscaled_big/1950_33912_id=cp131509_linz.jpg\"\n",
    "nk_furnitures = os.listdir(\"nk_collection_meubels_cleaned\")\n",
    "\n",
    "def compute_similarities(munich_img, nk_collection, path=\"nk_collection_meubels_cleaned\"):\n",
    "    \"\"\"\n",
    "    This function takes three arguments: \n",
    "    - munich_img, which is a single image from the Munich Database. \n",
    "    - nk_collection, this collection contains all the furnitures from the nk collection API. \n",
    "    - path, this is the path to the gray scaled nk collection.\n",
    "    \n",
    "    It then computes the feature descriptor for the munich image and all the images in the nk \\\n",
    "    collection. Afterwards takes the dot-product to get the dot-product similiarity. It then \\\n",
    "    saves the similarity and the two images as key-value pairs in a dictionary. \n",
    "    \"\"\"\n",
    "    \n",
    "    similarities = {}\n",
    "    for img in nk_collection:\n",
    "        img_path = os.path.join(path, img)\n",
    "        nk_img_feature_descriptor = normalize_features(extract_features(img_path).flatten())\n",
    "        munich_img_feature_descriptor = normalize_features(extract_features(munich_img).flatten())\n",
    "        similarity = np.dot(\n",
    "            nk_img_feature_descriptor,\n",
    "            munich_img_feature_descriptor\n",
    "        )\n",
    "        munich_img_name = munich_img[munich_img.rfind(\"/\")+1:]\n",
    "        similarities[(munich_img_name, img)] = similarity.item()\n",
    "        \n",
    "    return similarities\n",
    "    \n",
    "compute_similarities(munich_img, nk_furnitures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "05f6d45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best_model_state_dict = model.state_dict()\n",
    "#torch.save(best_model_state_dict, \"best2_vgg16_weights.pth\")\n",
    "best_model_state_dict = torch.load(\"best2_vgg16_weights.pth\")\n",
    "model.load_state_dict(best_model_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8a6fcd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('kast_nk.jpg', 'stoel_mccp.jpg'): 0.6604210138320923,\n",
       " ('kast_nk.jpg', 'tafel_mccp.jpg'): 0.6619276404380798,\n",
       " ('kast_nk.jpg', 'kast_mccp.jpg'): 0.8534377813339233,\n",
       " ('kast_nk.jpg', 'dressoir_mccp.jpg'): 0.8410069942474365,\n",
       " ('kast_nk.jpg', 'speeltafel_mccp.png'): 0.6869429349899292,\n",
       " ('speeltafel_nk.png', 'stoel_mccp.jpg'): 0.548683226108551,\n",
       " ('speeltafel_nk.png', 'tafel_mccp.jpg'): 0.5701091885566711,\n",
       " ('speeltafel_nk.png', 'kast_mccp.jpg'): 0.5543745756149292,\n",
       " ('speeltafel_nk.png', 'dressoir_mccp.jpg'): 0.5458159446716309,\n",
       " ('speeltafel_nk.png', 'speeltafel_mccp.png'): 0.6365468502044678,\n",
       " ('tafel_nk.jpg', 'stoel_mccp.jpg'): 0.4823276102542877,\n",
       " ('tafel_nk.jpg', 'tafel_mccp.jpg'): 0.5581287145614624,\n",
       " ('tafel_nk.jpg', 'kast_mccp.jpg'): 0.5177386999130249,\n",
       " ('tafel_nk.jpg', 'dressoir_mccp.jpg'): 0.4840072989463806,\n",
       " ('tafel_nk.jpg', 'speeltafel_mccp.png'): 0.5526392459869385,\n",
       " ('dressoir_nk.jpg', 'stoel_mccp.jpg'): 0.6967198848724365,\n",
       " ('dressoir_nk.jpg', 'tafel_mccp.jpg'): 0.6889641284942627,\n",
       " ('dressoir_nk.jpg', 'kast_mccp.jpg'): 0.7095674276351929,\n",
       " ('dressoir_nk.jpg', 'dressoir_mccp.jpg'): 0.7526007890701294,\n",
       " ('dressoir_nk.jpg', 'speeltafel_mccp.png'): 0.5907377600669861,\n",
       " ('stoel_nk.jpg', 'stoel_mccp.jpg'): 0.7785615921020508,\n",
       " ('stoel_nk.jpg', 'tafel_mccp.jpg'): 0.6855841875076294,\n",
       " ('stoel_nk.jpg', 'kast_mccp.jpg'): 0.7317004799842834,\n",
       " ('stoel_nk.jpg', 'dressoir_mccp.jpg'): 0.7492318153381348,\n",
       " ('stoel_nk.jpg', 'speeltafel_mccp.png'): 0.5899832844734192}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nk_testset = os.listdir(\"/home/hamid/Downloads/nk_testset\")\n",
    "munich_testset = os.listdir(\"/home/hamid/Downloads/munich_testset\")\n",
    "\n",
    "def compute_similarities_testsets(munich_testset, nk_testset, \n",
    "                                  munich_path=\"/home/hamid/Downloads/munich_testset\", \n",
    "                                  nk_path=\"/home/hamid/Downloads/nk_testset\"):\n",
    "    \"\"\"\n",
    "    This function takes four arguments: \n",
    "    - munich_testset, which contains 5 grayscaled images from the munich database.\n",
    "    - nk_testset, which contains 5 grayscaled images from the nk collection API.\n",
    "    - munich path, the path to the directory of the munich images. \n",
    "    - nk_path, the path to the directory of the nk images. \n",
    "    \n",
    "    It then computes the feature descriptors for the munich images and all the \\\n",
    "    nk collection images. Afterwards takes the dot-product to get the dot-product similiarity. \n",
    "    It then saves the similarity and the two images as key-value pairs in a dictionary. \n",
    "    \"\"\"\n",
    "    \n",
    "    similarities = {}\n",
    "    for nk_img in nk_testset:\n",
    "        nk_img_path = os.path.join(nk_path, nk_img)\n",
    "        for munich_img in munich_testset:\n",
    "            munich_img_path = os.path.join(munich_path, munich_img)\n",
    "            nk_img_feature_descriptor = normalize_features(extract_features(nk_img_path).flatten())\n",
    "            munich_img_feature_descriptor = normalize_features(extract_features(munich_img_path).flatten())\n",
    "            similarity = np.dot(\n",
    "                nk_img_feature_descriptor,\n",
    "                munich_img_feature_descriptor\n",
    "            )\n",
    "            similarities[(nk_img, munich_img)] = similarity.item()\n",
    "        \n",
    "    return similarities\n",
    "    \n",
    "sims = compute_similarities_testsets(munich_testset, nk_testset)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7ac078e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagee2 = cv2.imread(\"/home/hamid/Downloads/nk_testset/stoel_nk.jpg\", -1)\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8,8))\n",
    "e_image2 = clahe.apply(imagee2)\n",
    "_, thresh = cv2.threshold(e_image2, 127, 255, cv2.THRESH_BINARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f4fac5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "mask = np.ones(imagee2.shape, np.uint8)\n",
    "mask.fill(255)\n",
    "cv2.drawContours(mask, contours, 0, 0, -1)\n",
    "new_img = cv2.add(thresh, mask)\n",
    "kernel = np.ones((5,5), dtype=np.uint8)\n",
    "new_img = cv2.erode(new_img, kernel, 10)\n",
    "new_img = np.abs(np.max(new_img) - new_img)\n",
    "cv2.imshow(\"img\", new_img)\n",
    "cv2.imwrite(\"stoel_nk_preprocessed.jpg\", new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0498509c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stoel_mccp</th>\n",
       "      <th>tafel_mccp</th>\n",
       "      <th>kast_mccp</th>\n",
       "      <th>dressoir_mccp</th>\n",
       "      <th>speeltafel_mccp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kast_nk</th>\n",
       "      <td>0.660421</td>\n",
       "      <td>0.661928</td>\n",
       "      <td>0.853438</td>\n",
       "      <td>0.841007</td>\n",
       "      <td>0.686943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speeltafel_nk</th>\n",
       "      <td>0.548683</td>\n",
       "      <td>0.570109</td>\n",
       "      <td>0.554375</td>\n",
       "      <td>0.545816</td>\n",
       "      <td>0.636547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tafel_nk</th>\n",
       "      <td>0.482328</td>\n",
       "      <td>0.558129</td>\n",
       "      <td>0.517739</td>\n",
       "      <td>0.484007</td>\n",
       "      <td>0.552639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dressoir_nk</th>\n",
       "      <td>0.696720</td>\n",
       "      <td>0.688964</td>\n",
       "      <td>0.709567</td>\n",
       "      <td>0.752601</td>\n",
       "      <td>0.590738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoel_nk</th>\n",
       "      <td>0.778562</td>\n",
       "      <td>0.685584</td>\n",
       "      <td>0.731700</td>\n",
       "      <td>0.749232</td>\n",
       "      <td>0.589983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               stoel_mccp  tafel_mccp  kast_mccp  dressoir_mccp  \\\n",
       "kast_nk          0.660421    0.661928   0.853438       0.841007   \n",
       "speeltafel_nk    0.548683    0.570109   0.554375       0.545816   \n",
       "tafel_nk         0.482328    0.558129   0.517739       0.484007   \n",
       "dressoir_nk      0.696720    0.688964   0.709567       0.752601   \n",
       "stoel_nk         0.778562    0.685584   0.731700       0.749232   \n",
       "\n",
       "               speeltafel_mccp  \n",
       "kast_nk               0.686943  \n",
       "speeltafel_nk         0.636547  \n",
       "tafel_nk              0.552639  \n",
       "dressoir_nk           0.590738  \n",
       "stoel_nk              0.589983  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_table(sims):\n",
    "    \"\"\"\n",
    "    This function takes the output produced by either the compute_similarities \\ \n",
    "    or compute_similarities_testsets function, and returns a pandas dataframe/table \\\n",
    "    and also saves it in excel.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    rows = []\n",
    "\n",
    "    for key, value in sims.items():\n",
    "        if key[0] not in data:\n",
    "            data[key[0]] = []\n",
    "        if key[1] not in rows:\n",
    "            rows.append(key[1])\n",
    "        data[key[0]].append(value)\n",
    "        \n",
    "    data = {key[:key.rfind(\".\")]:value for key, value in data.items()}\n",
    "    rows = [row[:row.rfind(\".\")] for row in rows]\n",
    "        \n",
    "    df = pd.DataFrame(data, index=rows)\n",
    "    #df.to_excel('output.xlsx')\n",
    "    return df.T\n",
    "    \n",
    "get_table(sims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
