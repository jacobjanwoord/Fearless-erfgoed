{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653a9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93659f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()\n",
    "optional_path = \"Downloads\"\n",
    "relative_path_nk = \"nk_collection_meubels_cleaned\"\n",
    "relative_path_munich = \"scraped_images_grayscaled_big\"\n",
    "abs_path_nk = os.path.join(base_dir, optional_path, relative_path_nk)\n",
    "abs_path_munich = os.path.join(base_dir, optional_path, relative_path_munich)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1afd7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "model.features[0] = nn.Conv2d(1,64,kernel_size=(3,3), stride=(1,1),padding=(1,1))\n",
    "model = nn.Sequential(*[*list(model.children())[:-1][0][:-10]])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    This function takes a path to a single image, it then resizes it to size 50x50 \\\n",
    "    and normalizes it to the range [0,1]. Lastly, it adds an extra dimension to the image \\\n",
    "    which represents the batch size. These steps are needed, because we want to pass the image \\\n",
    "    to a CNN. \n",
    "    \"\"\"\n",
    "    \n",
    "    img = cv2.imread(image_path, -1)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8,8))\n",
    "    img = clahe.apply(img)\n",
    "    _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    mask = np.ones(img.shape, np.uint8)\n",
    "    mask.fill(255)\n",
    "    cv2.drawContours(mask, contours, 0, 0, -1)\n",
    "    img = cv2.add(thresh, mask)\n",
    "    kernel = np.ones((5,5), dtype=np.uint8)\n",
    "    img = cv2.erode(img, kernel, 10)\n",
    "    img = np.abs(np.max(img) - img)\n",
    "    \n",
    "    img = cv2.resize(img, (50, 50), interpolation=cv2.INTER_AREA)\n",
    "    preprocess = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    img = preprocess(img).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "def extract_features(image_path):\n",
    "    \"\"\"\n",
    "    This function takes a path to a single image, it then preprocesses the image with the \\\n",
    "    function preprocess_image. Afterwards it passes the image to the pretrained CNN to extract \\\n",
    "    a feature descriptor. \n",
    "    \"\"\"\n",
    "    \n",
    "    img = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "    return features.squeeze(0).numpy()\n",
    "\n",
    "def normalize_features(features):\n",
    "    \"\"\"\n",
    "    This function takes the feature descriptor and normalizes it. This is needed as we want \\\n",
    "    to compute the dot-product similarity between feature descriptors of different images. \\\n",
    "    And for similarity it is convenient to have all pixels on the same scale without too \\\n",
    "    much magnitude differences and this also ensures stability. \n",
    "    \"\"\"\n",
    "    \n",
    "    return features / np.linalg.norm(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05f6d45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best_model_state_dict = model.state_dict()\n",
    "#torch.save(best_model_state_dict, \"best2_vgg16_weights.pth\")\n",
    "best_model_state_dict = torch.load(\"best2_vgg16_weights.pth\")\n",
    "model.load_state_dict(best_model_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b380a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nk_img = os.path.join(base_dir, optional_path, \"nk_testset\", \"kast_nk.jpg\")\n",
    "munich_imgs = os.listdir(abs_path_munich)\n",
    "\n",
    "def compute_similarities(nk_img, munich_imgs, \n",
    "                         path=abs_path_munich):\n",
    "    \"\"\"\n",
    "    This function takes three arguments: \n",
    "    - nk_img, which is a single image from the nk collection. \n",
    "    - munich_imgs, this contains all images from the Munich Database. \n",
    "    - path, this is the path to the gray scaled Munich Database.\n",
    "    \n",
    "    It then computes the feature descriptor for the nk collection image and all the images in the \\\n",
    "    Munich Database. Afterwards takes the dot-product to get the dot-product similiarity. It then \\\n",
    "    saves the similarity and the two images as key-value pairs in a dictionary. \n",
    "    \"\"\"\n",
    "    \n",
    "    similarities = {}\n",
    "    nk_img_feature_descriptor = normalize_features(extract_features(nk_img).flatten())\n",
    "    for img in munich_imgs:\n",
    "        img_path = os.path.join(path, img)\n",
    "        munich_img_feature_descriptor = normalize_features(extract_features(img_path).flatten())\n",
    "        similarity = np.dot(\n",
    "            nk_img_feature_descriptor,\n",
    "            munich_img_feature_descriptor\n",
    "        )\n",
    "        munich_img_name = img_path[img_path.rfind(\"/\")+1:]\n",
    "        nk_img_name = nk_img[nk_img.rfind(\"/\")+1:]\n",
    "        similarities[(nk_img_name, munich_img_name)] = similarity.item()\n",
    "        \n",
    "    return similarities\n",
    "    \n",
    "sims_complete = compute_similarities(nk_img, munich_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d8829c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "nk_no_back = os.listdir(\"/home/hamid/Downloads/nk_no_back\")\n",
    "img = cv2.imread(os.path.join(base_dir, optional_path, \"nk_no_back\", \"meubel_22.jpg\"), -1)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\"\"\"\n",
    "cv2.imshow(\"img\", cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\"\"\"\n",
    "_, thresh = cv2.threshold(img, 110, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow(\"img\", thresh)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f68dd38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('kast_nk.jpg', '0270_1450_id=cp131480_linz.jpg'), 0.9085464477539062),\n",
       " (('kast_nk.jpg', '1083_7988-67_id=cp173744_badv.jpg'), 0.9084235429763794),\n",
       " (('kast_nk.jpg', '0858_5593-1_id=cp168777_badv.jpg'), 0.9077965021133423),\n",
       " (('kast_nk.jpg', '0718_4316_id=cp131435_linz.jpg'), 0.9069232940673828),\n",
       " (('kast_nk.jpg', '1085_7988-96_id=cp173776_badv.jpg'), 0.9068341255187988),\n",
       " (('kast_nk.jpg', '0107_613-4_id=cp169665_badv.jpg'), 0.9065513014793396),\n",
       " (('kast_nk.jpg', '0396_1794-8_id=cp145569_badv.jpg'), 0.9059417843818665),\n",
       " (('kast_nk.jpg', '1070_7951-24_id=cp173281_badv.jpg'), 0.9056847095489502),\n",
       " (('kast_nk.jpg', '0228_1252-11_id=cp138846_badv.jpg'), 0.9056298732757568),\n",
       " (('kast_nk.jpg', '0082_420-6_id=cp160089_badv.jpg'), 0.9056044816970825)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = {k:v for k,v in sims_complete.items() if v > 0.82}\n",
    "sorted_filtered = sorted(filtered.items(), key=lambda item: item[1], reverse=True)[20:30]\n",
    "sorted_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "274ccfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join(abs_path_munich, '1083_7988-67_id=cp173744_badv.jpg'), -1)\n",
    "cv2.imshow(\"img\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a6fcd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "nk_testset = os.listdir(os.path.join(base_dir, optional_path, \"nk_testset\"))\n",
    "munich_testset = os.listdir(os.path.join(base_dir, optional_path, \"munich_testset\"))\n",
    "\n",
    "def compute_similarities_testsets(munich_testset, nk_testset, \n",
    "                                  munich_path=os.path.join(base_dir, optional_path, \"munich_testset\"), \n",
    "                                  nk_path=os.path.join(base_dir, optional_path, \"nk_testset\")):\n",
    "    \"\"\"\n",
    "    This function takes four arguments: \n",
    "    - munich_testset, which contains 5 grayscaled images from the munich database.\n",
    "    - nk_testset, which contains 5 grayscaled images from the nk collection API.\n",
    "    - munich path, the path to the directory of the munich images. \n",
    "    - nk_path, the path to the directory of the nk images. \n",
    "    \n",
    "    It then computes the feature descriptors for the munich images and all the \\\n",
    "    nk collection images. Afterwards takes the dot-product to get the dot-product similiarity. \n",
    "    It then saves the similarity and the two images as key-value pairs in a dictionary. \n",
    "    \"\"\"\n",
    "    \n",
    "    similarities = {}\n",
    "    for nk_img in nk_testset:\n",
    "        nk_img_path = os.path.join(nk_path, nk_img)\n",
    "        for munich_img in munich_testset:\n",
    "            munich_img_path = os.path.join(munich_path, munich_img)\n",
    "            nk_img_feature_descriptor = normalize_features(extract_features(nk_img_path).flatten())\n",
    "            munich_img_feature_descriptor = normalize_features(extract_features(munich_img_path).flatten())\n",
    "            similarity = np.dot(\n",
    "                nk_img_feature_descriptor,\n",
    "                munich_img_feature_descriptor\n",
    "            )\n",
    "            similarities[(nk_img, munich_img)] = similarity.item()\n",
    "        \n",
    "    return similarities\n",
    "    \n",
    "sims = compute_similarities_testsets(munich_testset, nk_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0498509c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stoel_mccp</th>\n",
       "      <th>tafel_mccp</th>\n",
       "      <th>kast_mccp</th>\n",
       "      <th>dressoir_mccp</th>\n",
       "      <th>speeltafel_mccp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kast_nk</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speeltafel_nk</th>\n",
       "      <td>0.549</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tafel_nk</th>\n",
       "      <td>0.482</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dressoir_nk</th>\n",
       "      <td>0.697</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoel_nk</th>\n",
       "      <td>0.779</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               stoel_mccp  tafel_mccp  kast_mccp  dressoir_mccp  \\\n",
       "kast_nk             0.660       0.662      0.853          0.841   \n",
       "speeltafel_nk       0.549       0.570      0.554          0.546   \n",
       "tafel_nk            0.482       0.558      0.518          0.484   \n",
       "dressoir_nk         0.697       0.689      0.710          0.753   \n",
       "stoel_nk            0.779       0.686      0.732          0.749   \n",
       "\n",
       "               speeltafel_mccp  \n",
       "kast_nk                  0.687  \n",
       "speeltafel_nk            0.637  \n",
       "tafel_nk                 0.553  \n",
       "dressoir_nk              0.591  \n",
       "stoel_nk                 0.590  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_table(sims):\n",
    "    \"\"\"\n",
    "    This function takes the output produced by either the compute_similarities \\ \n",
    "    or compute_similarities_testsets function, and returns a pandas dataframe/table \\\n",
    "    and also saves it in excel.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    rows = []\n",
    "\n",
    "    for key, value in sims.items():\n",
    "        if key[0] not in data:\n",
    "            data[key[0]] = []\n",
    "        if key[1] not in rows:\n",
    "            rows.append(key[1])\n",
    "        value = np.round(value, 3)\n",
    "        data[key[0]].append(value)\n",
    "        \n",
    "    data = {key[:key.rfind(\".\")]:value for key, value in data.items()}\n",
    "    rows = [row[:row.rfind(\".\")] for row in rows]\n",
    "        \n",
    "    df = pd.DataFrame(data, index=rows)\n",
    "    #df.to_excel('output.xlsx')\n",
    "    return df.T\n",
    "    \n",
    "get_table(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ac72493",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torchvision.models.vgg16(pretrained=True)\n",
    "model2.features[0] = nn.Conv2d(1,64,kernel_size=(3,3), stride=(1,1),padding=(1,1))\n",
    "model2.classifier[-1] = nn.Linear(model2.classifier[-1].in_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72238cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15\n",
      "Train Loss: 0.6964\n",
      "Validation Loss: 0.6925\n",
      "Validation Accuracy: 0.5310\n",
      "Epoch: 2/15\n",
      "Train Loss: 0.6945\n",
      "Validation Loss: 0.6940\n",
      "Validation Accuracy: 0.4690\n",
      "Epoch: 3/15\n",
      "Train Loss: 0.6941\n",
      "Validation Loss: 0.6940\n",
      "Validation Accuracy: 0.4690\n",
      "Epoch: 4/15\n",
      "Train Loss: 0.6941\n",
      "Validation Loss: 0.6980\n",
      "Validation Accuracy: 0.4690\n",
      "Epoch: 5/15\n",
      "Train Loss: 0.6952\n",
      "Validation Loss: 0.6916\n",
      "Validation Accuracy: 0.5310\n",
      "Epoch: 6/15\n",
      "Train Loss: 0.6960\n",
      "Validation Loss: 0.7036\n",
      "Validation Accuracy: 0.4690\n",
      "Epoch: 7/15\n",
      "Train Loss: 0.6948\n",
      "Validation Loss: 0.6950\n",
      "Validation Accuracy: 0.4690\n",
      "Epoch: 8/15\n",
      "Train Loss: 0.6946\n",
      "Validation Loss: 0.6989\n",
      "Validation Accuracy: 0.4690\n",
      "Epoch: 9/15\n",
      "Train Loss: 0.6961\n",
      "Validation Loss: 0.6919\n",
      "Validation Accuracy: 0.5310\n",
      "Epoch: 10/15\n",
      "Train Loss: 0.6931\n",
      "Validation Loss: 0.7018\n",
      "Validation Accuracy: 0.4690\n",
      "Epoch: 11/15\n",
      "Train Loss: 0.6939\n",
      "Validation Loss: 0.7011\n",
      "Validation Accuracy: 0.4690\n",
      "Epoch: 12/15\n",
      "Train Loss: 0.6941\n",
      "Validation Loss: 0.6920\n",
      "Validation Accuracy: 0.5310\n",
      "Epoch: 13/15\n",
      "Train Loss: 0.6945\n",
      "Validation Loss: 0.6938\n",
      "Validation Accuracy: 0.4690\n",
      "Epoch: 14/15\n",
      "Train Loss: 0.6939\n",
      "Validation Loss: 0.6955\n",
      "Validation Accuracy: 0.4690\n",
      "Epoch: 15/15\n",
      "Train Loss: 0.6939\n",
      "Validation Loss: 0.6969\n",
      "Validation Accuracy: 0.4690\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import random\n",
    "import shutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "def select_subset_mun(sample_size, source_dir=abs_path_munich, \n",
    "                     target_dir=os.path.join(base_dir, optional_path, \"mun_cleaned\")):\n",
    "    mun_imgs = os.listdir(abs_path_munich)\n",
    "    subset_mun = random.sample(mun_imgs, sample_size)\n",
    "    for img in subset_mun:\n",
    "        source_img = os.path.join(source_dir, img)\n",
    "        target_img = os.path.join(target_dir, img)\n",
    "        shutil.copyfile(source_img, target_img)\n",
    "        \n",
    "def delete_subset_mun(sample_size, dir_path=os.path.join(base_dir, optional_path, \"mun_cleaned\")):\n",
    "    imgs = os.listdir(dir_path)\n",
    "    random_subset = random.sample(imgs, sample_size)\n",
    "    for img in random_subset:\n",
    "        img_path = os.path.join(dir_path, img)\n",
    "        if os.path.isfile(img_path) or os.path.islink(img_path):\n",
    "            os.unlink(img_path)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path, -1)\n",
    "        img = cv2.resize(img, (50,50), interpolation=cv2.INTER_AREA)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "def load_data(base_dir=base_dir, optional_path=optional_path):\n",
    "    nk_imgs = os.listdir(os.path.join(base_dir, \"nk_collection_meubels_cleaned\"))\n",
    "    mun_imgs = os.listdir(os.path.join(base_dir, optional_path, \"mun_cleaned\"))\n",
    "    nk_imgs_paths = [os.path.join(\n",
    "        base_dir, \"nk_collection_meubels_cleaned\", img\n",
    "    ) for img in nk_imgs]\n",
    "    mun_imgs_paths = [os.path.join(\n",
    "        base_dir, optional_path, \"mun_cleaned\", img\n",
    "    ) for img in mun_imgs]\n",
    "    \n",
    "    img_paths = nk_imgs_paths + mun_imgs_paths\n",
    "    labels = [0] * len(nk_imgs_paths) + [1] * len(mun_imgs_paths)\n",
    "    \n",
    "    return img_paths, labels\n",
    "\n",
    "image_paths, labels = load_data()\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(image_paths, labels, \n",
    "                                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(train_paths, train_labels, transform=transform)\n",
    "val_dataset = CustomDataset(val_paths, val_labels, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.05)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=15):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for (images, labels) in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for (images, labels) in val_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        print(\n",
    "            f\"Epoch: {epoch+1}/{num_epochs}\",\n",
    "            f\"Train Loss: {running_loss/len(train_loader):.4f}\",\n",
    "            f\"Validation Loss: {val_loss/len(val_loader):.4f}\",\n",
    "            f\"Validation Accuracy: {correct/total:.4f}\",\n",
    "            sep=\"\\n\"\n",
    "        )\n",
    "\n",
    "train_model(model2, train_loader, val_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2dac1062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3444, 0.6556]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "nk_no_back = os.listdir(os.path.join(base_dir, optional_path, \"nk_no_back\"))\n",
    "mc_no_back = os.listdir(os.path.join(base_dir, optional_path, \"mc_no_back\"))\n",
    "test_img = os.path.join(base_dir, optional_path, \"nk_no_back\", nk_no_back[0])\n",
    "\n",
    "def preprocess_image2(image_path):\n",
    "    img = cv2.imread(image_path, -1)\n",
    "    img = cv2.resize(img, (50,50), interpolation=cv2.INTER_AREA)\n",
    "    preprocess = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    img = preprocess(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "with torch.no_grad():\n",
    "    img = preprocess_image2(test_img)\n",
    "    outputs = model2(img)\n",
    "    outputs = F.softmax(outputs, 1)\n",
    "    print(outputs)\n",
    "    pred = torch.argmax(outputs).item()\n",
    "    label = (pred == 0)\n",
    "    print(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c19d8e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_state_dict = model2.state_dict()\n",
    "torch.save(model2_state_dict, \"classify_vgg16.pth\")\n",
    "model2_state_dict = torch.load(\"classify_vgg16.pth\")\n",
    "model2.load_state_dict(model2_state_dict)\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f797690f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
