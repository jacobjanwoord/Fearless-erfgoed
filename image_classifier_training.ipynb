{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n",
    "# ds = deeplake.load('hub://activeloop/wiki-art')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HINT: Please forward the port - 53259 to your local machine, if you are running on the cloud.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"90%\"\n",
       "            height=\"800\"\n",
       "            src=\"https://app.activeloop.ai/visualizer/hub?url=hub://activeloop/wiki-art&token=PUBLIC_TOKEN_______________________________________________________________________________________________________________________________________________________\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1af8084b9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'dataset_visualizer'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "ds.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\AppData\\Roaming\\Python\\Python310\\site-packages\\deeplake\\integrations\\pytorch\\common.py:137: UserWarning: Decode method for tensors ['images'] is defaulting to numpy. Please consider specifying a decode_method in .pytorch() that maximizes the data preprocessing speed based on your transformation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloader_paintings = ds.pytorch(num_workers=0, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deeplake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m----> 4\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mdeeplake\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhub://activeloop/imagenet-train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m tform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      6\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToPILImage(), \u001b[38;5;66;03m# Must convert to PIL image for subsequent operations to run\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomRotation(\u001b[38;5;241m20\u001b[39m), \u001b[38;5;66;03m# Image augmentation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),\n\u001b[0;32m     11\u001b[0m ])\n\u001b[0;32m     14\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mpytorch(num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, transform \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m: tform, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'deeplake' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import transforms\n",
    "\n",
    "\n",
    "ds = deeplake.load(\"hub://activeloop/imagenet-train\")\n",
    "tform = transforms.Compose([\n",
    "    transforms.ToPILImage(), # Must convert to PIL image for subsequent operations to run\n",
    "    transforms.RandomRotation(20), # Image augmentation\n",
    "    transforms.ToTensor(), # Must convert to pytorch tensor for subsequent operations to run\n",
    "    transforms.Lambda(lambda x: x.repeat(int(3/x.shape[0]), 1, 1)), # Some images are grayscale, so we need to add channels\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "dataloader = ds.pytorch(num_workers=0, batch_size=4, transform = {'images': tform, 'labels': None}, shuffle = True)\n",
    "dataloader_other = ds.pytorch(num_workers=0, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89435c18aa42473a8234fa2f782bd5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "ds = load_dataset(\"huggan/wikiart\", split='train[0:6000]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6d2541b934481bbd3c108e8579ab8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_images_ds = load_dataset(\"random_images/dataset/train\", split='train[0:6000]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds)\n",
    "ds = ds.rename_column(\"artist\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(len(ds)):\n",
    "#     ds[i]['labels'] = 'painting'\n",
    "\n",
    "ds[10]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1423x1382>,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(len(ds)):\n",
    "#     ds[i]['labels'] = 'painting'\n",
    "\n",
    "ds = ds.remove_columns(\"label\").add_column(\"label\", [0]*len(ds))\n",
    "# ds = ds.remove_columns('genre')\n",
    "# ds = ds.remove_columns('style')\n",
    "ds[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=80x60>,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_images_ds = r_images_ds.remove_columns(\"label\").add_column(\"label\", [1]*len(ds))\n",
    "r_images_ds[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = concatenate_datasets([r_images_ds, ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import albumentations\n",
    "import numpy as np\n",
    "\n",
    "transform = albumentations.Compose([\n",
    "    albumentations.RandomCrop(width=64, height=64),\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.RandomBrightnessContrast(p=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        transform(image=np.array(image))[\"image\"] for image in examples[\"image\"]\n",
    "    ]\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d38f943d60>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = image_dataset[0][\"pixel_values\"]\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"microsoft/swin-tiny-patch4-window7-224\" # pre-trained model from which to fine-tune\n",
    "batch_size = 32 # batch size for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"_valid_processor_keys\": [\n",
       "    \"images\",\n",
       "    \"do_resize\",\n",
       "    \"size\",\n",
       "    \"resample\",\n",
       "    \"do_rescale\",\n",
       "    \"rescale_factor\",\n",
       "    \"do_normalize\",\n",
       "    \"image_mean\",\n",
       "    \"image_std\",\n",
       "    \"return_tensors\",\n",
       "    \"data_format\",\n",
       "    \"input_data_format\"\n",
       "  ],\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor  = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
    "image_processor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     CenterCrop,\n\u001b[0;32m      3\u001b[0m     Compose,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     Grayscale\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m normalize \u001b[38;5;241m=\u001b[39m Normalize(mean\u001b[38;5;241m=\u001b[39m\u001b[43mimage_processor\u001b[49m\u001b[38;5;241m.\u001b[39mimage_mean, std\u001b[38;5;241m=\u001b[39mimage_processor\u001b[38;5;241m.\u001b[39mimage_std)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m image_processor\u001b[38;5;241m.\u001b[39msize:\n\u001b[0;32m     14\u001b[0m     size \u001b[38;5;241m=\u001b[39m (image_processor\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m], image_processor\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_processor' is not defined"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    "    Grayscale\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "if \"height\" in image_processor.size:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = image_processor.size.get(\"longest_edge\")\n",
    "\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(crop_size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "            Grayscale,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "splits = image_dataset.train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [0, 1]\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "id2label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    "    device_map='cuda',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-eurosat\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a77fbe65c7401596fe34b874c8710a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408f5f2d28d64f16aacedf20de257d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7346, 'grad_norm': 4.7906975746154785, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2386, 'grad_norm': 1.2752577066421509, 'learning_rate': 3.846153846153846e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0087, 'grad_norm': 0.0010421701008453965, 'learning_rate': 4.911504424778761e-05, 'epoch': 0.36}\n",
      "{'loss': 0.005, 'grad_norm': 12.082448959350586, 'learning_rate': 4.690265486725664e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0112, 'grad_norm': 0.014315508306026459, 'learning_rate': 4.469026548672566e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0092, 'grad_norm': 18.838348388671875, 'learning_rate': 4.247787610619469e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0231, 'grad_norm': 0.005520842038094997, 'learning_rate': 4.026548672566372e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0008, 'grad_norm': 0.0006030216463841498, 'learning_rate': 3.8053097345132744e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1e00d5baa14406bdb3f7b973d36585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004591928329318762, 'eval_accuracy': 0.9975, 'eval_runtime': 112.2718, 'eval_samples_per_second': 10.688, 'eval_steps_per_second': 0.338, 'epoch': 0.99}\n",
      "{'loss': 0.0102, 'grad_norm': 0.0038931695744395256, 'learning_rate': 3.5840707964601774e-05, 'epoch': 1.07}\n",
      "{'loss': 0.001, 'grad_norm': 0.0007370581733994186, 'learning_rate': 3.3628318584070804e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# rest is optional but nice to have\u001b[39;00m\n\u001b[0;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:1876\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1873\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1874\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[0;32m   1875\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[1;32m-> 1876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1877\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1883\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2178\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2175\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2177\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   2179\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\accelerate\\data_loader.py:464\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[1;32m--> 464\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:2870\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[1;34m(self, keys)\u001b[0m\n\u001b[0;32m   2868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[0;32m   2869\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2870\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2871\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[0;32m   2872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:2866\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:2851\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2849\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2850\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[1;32m-> 2851\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\formatting\\formatting.py:633\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    631\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\formatting\\formatting.py:401\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\formatting\\formatting.py:516\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    514\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[0;32m    515\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[1;32m--> 516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 41\u001b[0m, in \u001b[0;36mpreprocess_train\u001b[1;34m(example_batch)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_train\u001b[39m(example_batch):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply train_transforms across a batch.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     example_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     42\u001b[0m         train_transforms(image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m example_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     43\u001b[0m     ]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m example_batch\n",
      "Cell \u001b[1;32mIn[40], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_train\u001b[39m(example_batch):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply train_transforms across a batch.\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     example_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 42\u001b[0m         \u001b[43mtrain_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m example_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     43\u001b[0m     ]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m example_batch\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "# rest is optional but nice to have\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8977f027ba0a48f5b7daaaaa4a959efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.006888017524033785, 'eval_accuracy': 0.9966666666666667, 'eval_runtime': 136.0728, 'eval_samples_per_second': 8.819, 'eval_steps_per_second': 0.279, 'epoch': 1.2}\n",
      "***** eval metrics *****\n",
      "  epoch                   =     1.1953\n",
      "  eval_accuracy           =     0.9967\n",
      "  eval_loss               =     0.0069\n",
      "  eval_runtime            = 0:02:16.07\n",
      "  eval_samples_per_second =      8.819\n",
      "  eval_steps_per_second   =      0.279\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "# some nice to haves:\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7e41cbc21f4a61b74c6177b49c4a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/110M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/JacobJan/swin-tiny-patch4-window7-224-finetuned-eurosat/commit/b51e215487c5a9156c38dad3924631efe4836ac4', commit_message='End of training', commit_description='', oid='b51e215487c5a9156c38dad3924631efe4836ac4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
