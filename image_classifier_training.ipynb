{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n",
    "# ds = deeplake.load('hub://activeloop/wiki-art')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HINT: Please forward the port - 53259 to your local machine, if you are running on the cloud.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"90%\"\n",
       "            height=\"800\"\n",
       "            src=\"https://app.activeloop.ai/visualizer/hub?url=hub://activeloop/wiki-art&token=PUBLIC_TOKEN_______________________________________________________________________________________________________________________________________________________\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1af8084b9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'dataset_visualizer'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "ds.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\AppData\\Roaming\\Python\\Python310\\site-packages\\deeplake\\integrations\\pytorch\\common.py:137: UserWarning: Decode method for tensors ['images'] is defaulting to numpy. Please consider specifying a decode_method in .pytorch() that maximizes the data preprocessing speed based on your transformation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloader_paintings = ds.pytorch(num_workers=0, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deeplake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m----> 4\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mdeeplake\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhub://activeloop/imagenet-train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m tform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      6\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToPILImage(), \u001b[38;5;66;03m# Must convert to PIL image for subsequent operations to run\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomRotation(\u001b[38;5;241m20\u001b[39m), \u001b[38;5;66;03m# Image augmentation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),\n\u001b[0;32m     11\u001b[0m ])\n\u001b[0;32m     14\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mpytorch(num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, transform \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m: tform, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'deeplake' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import transforms\n",
    "\n",
    "\n",
    "ds = deeplake.load(\"hub://activeloop/imagenet-train\")\n",
    "tform = transforms.Compose([\n",
    "    transforms.ToPILImage(), # Must convert to PIL image for subsequent operations to run\n",
    "    transforms.RandomRotation(20), # Image augmentation\n",
    "    transforms.ToTensor(), # Must convert to pytorch tensor for subsequent operations to run\n",
    "    transforms.Lambda(lambda x: x.repeat(int(3/x.shape[0]), 1, 1)), # Some images are grayscale, so we need to add channels\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "dataloader = ds.pytorch(num_workers=0, batch_size=4, transform = {'images': tform, 'labels': None}, shuffle = True)\n",
    "dataloader_other = ds.pytorch(num_workers=0, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f88b626a0ca46c3a1f5e9dd3386cd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "ds = load_dataset(\"huggan/wikiart\", split='train[0:6000]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec2ba33b25545b7b4952f2a198c1325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_images_ds = load_dataset(\"random_images/dataset/train\", split='train[0:6000]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds)\n",
    "ds = ds.rename_column(\"artist\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(len(ds)):\n",
    "#     ds[i]['labels'] = 'painting'\n",
    "\n",
    "ds[10]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1423x1382>,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(len(ds)):\n",
    "#     ds[i]['labels'] = 'painting'\n",
    "\n",
    "ds = ds.remove_columns(\"label\").add_column(\"label\", [0]*len(ds))\n",
    "ds = ds.remove_columns('genre')\n",
    "ds = ds.remove_columns('style')\n",
    "ds[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=80x60>,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_images_ds = r_images_ds.add_column(\"label\", [1]*len(ds))\n",
    "r_images_ds[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = concatenate_datasets([r_images_ds, ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import albumentations\n",
    "import numpy as np\n",
    "\n",
    "transform = albumentations.Compose([\n",
    "    albumentations.RandomCrop(width=64, height=64),\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.RandomBrightnessContrast(p=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        transform(image=np.array(image))[\"image\"] for image in examples[\"image\"]\n",
    "    ]\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x247a0e1af80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = image_dataset[0][\"pixel_values\"]\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\AppData\\Local\\Temp\\ipykernel_13504\\272650354.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\", trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"microsoft/swin-tiny-patch4-window7-224\" # pre-trained model from which to fine-tune\n",
    "batch_size = 32 # batch size for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"_valid_processor_keys\": [\n",
       "    \"images\",\n",
       "    \"do_resize\",\n",
       "    \"size\",\n",
       "    \"resample\",\n",
       "    \"do_rescale\",\n",
       "    \"rescale_factor\",\n",
       "    \"do_normalize\",\n",
       "    \"image_mean\",\n",
       "    \"image_std\",\n",
       "    \"return_tensors\",\n",
       "    \"data_format\",\n",
       "    \"input_data_format\"\n",
       "  ],\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor  = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
    "image_processor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "if \"height\" in image_processor.size:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = image_processor.size.get(\"longest_edge\")\n",
    "\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(crop_size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "            Grayscale,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "splits = image_dataset.train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [0, 1]\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "id2label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.3.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m get_device_map()  \u001b[38;5;66;03m# 'cpu'\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForImageClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mmodel_checkpoint\u001b[49m, \n\u001b[0;32m     10\u001b[0m     label2id\u001b[38;5;241m=\u001b[39mlabel2id,\n\u001b[0;32m     11\u001b[0m     id2label\u001b[38;5;241m=\u001b[39mid2label,\n\u001b[0;32m     12\u001b[0m     ignore_mismatched_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# provide this in case you're planning to fine-tune an already fine-tuned checkpoint\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "def get_device_map() -> str:\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "device = get_device_map()  # 'cpu'\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    "    device_map='cpu',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-eurosat\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a77fbe65c7401596fe34b874c8710a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408f5f2d28d64f16aacedf20de257d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7346, 'grad_norm': 4.7906975746154785, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2386, 'grad_norm': 1.2752577066421509, 'learning_rate': 3.846153846153846e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0087, 'grad_norm': 0.0010421701008453965, 'learning_rate': 4.911504424778761e-05, 'epoch': 0.36}\n",
      "{'loss': 0.005, 'grad_norm': 12.082448959350586, 'learning_rate': 4.690265486725664e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0112, 'grad_norm': 0.014315508306026459, 'learning_rate': 4.469026548672566e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0092, 'grad_norm': 18.838348388671875, 'learning_rate': 4.247787610619469e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0231, 'grad_norm': 0.005520842038094997, 'learning_rate': 4.026548672566372e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0008, 'grad_norm': 0.0006030216463841498, 'learning_rate': 3.8053097345132744e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1e00d5baa14406bdb3f7b973d36585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004591928329318762, 'eval_accuracy': 0.9975, 'eval_runtime': 112.2718, 'eval_samples_per_second': 10.688, 'eval_steps_per_second': 0.338, 'epoch': 0.99}\n",
      "{'loss': 0.0102, 'grad_norm': 0.0038931695744395256, 'learning_rate': 3.5840707964601774e-05, 'epoch': 1.07}\n",
      "{'loss': 0.001, 'grad_norm': 0.0007370581733994186, 'learning_rate': 3.3628318584070804e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# rest is optional but nice to have\u001b[39;00m\n\u001b[0;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:1876\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1873\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1874\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[0;32m   1875\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[1;32m-> 1876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1877\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1883\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2178\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2175\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2177\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   2179\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\accelerate\\data_loader.py:464\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[1;32m--> 464\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:2870\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[1;34m(self, keys)\u001b[0m\n\u001b[0;32m   2868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[0;32m   2869\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2870\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2871\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[0;32m   2872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:2866\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:2851\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2849\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2850\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[1;32m-> 2851\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\formatting\\formatting.py:633\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    631\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\formatting\\formatting.py:401\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\formatting\\formatting.py:516\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    514\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[0;32m    515\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[1;32m--> 516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 41\u001b[0m, in \u001b[0;36mpreprocess_train\u001b[1;34m(example_batch)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_train\u001b[39m(example_batch):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply train_transforms across a batch.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     example_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     42\u001b[0m         train_transforms(image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m example_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     43\u001b[0m     ]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m example_batch\n",
      "Cell \u001b[1;32mIn[40], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_train\u001b[39m(example_batch):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply train_transforms across a batch.\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     example_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 42\u001b[0m         \u001b[43mtrain_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m example_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     43\u001b[0m     ]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m example_batch\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "# rest is optional but nice to have\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8977f027ba0a48f5b7daaaaa4a959efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.006888017524033785, 'eval_accuracy': 0.9966666666666667, 'eval_runtime': 136.0728, 'eval_samples_per_second': 8.819, 'eval_steps_per_second': 0.279, 'epoch': 1.2}\n",
      "***** eval metrics *****\n",
      "  epoch                   =     1.1953\n",
      "  eval_accuracy           =     0.9967\n",
      "  eval_loss               =     0.0069\n",
      "  eval_runtime            = 0:02:16.07\n",
      "  eval_samples_per_second =      8.819\n",
      "  eval_steps_per_second   =      0.279\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "# some nice to haves:\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7e41cbc21f4a61b74c6177b49c4a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/110M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/JacobJan/swin-tiny-patch4-window7-224-finetuned-eurosat/commit/b51e215487c5a9156c38dad3924631efe4836ac4', commit_message='End of training', commit_description='', oid='b51e215487c5a9156c38dad3924631efe4836ac4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
